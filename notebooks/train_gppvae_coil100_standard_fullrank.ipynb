{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c8a35916",
   "metadata": {},
   "source": [
    "# GP-VAE Training on COIL-100 (Standard Task) - FullRank Kernel\n",
    "\n",
    "This notebook trains **GP-VAE** on COIL-100 dataset using the **FullRank kernel** with **early stopping**.\n",
    "\n",
    "## Kernel: FullRank\n",
    "- **Nonparametric kernel** with full covariance matrix\n",
    "- K = LL^T where L is learned directly\n",
    "- **O(Q¬≤) parameters** where Q = number of views\n",
    "- **Best for**: Maximum flexibility, capturing any covariance structure\n",
    "\n",
    "## Dataset Info:\n",
    "- **COIL-100**: 100 objects √ó 18 views (every 20¬∞: 0¬∞, 20¬∞, ..., 340¬∞)\n",
    "- **Image size**: 128√ó128√ó3 RGB\n",
    "- **Task**: Standard split (random train/val/test)\n",
    "\n",
    "## Prerequisites:\n",
    "- ‚úÖ Trained VAE weights from `train_vae_colab_standard.ipynb`\n",
    "- ‚úÖ COIL-100 data file: `data/coil100/coil100_task1_standard.h5`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c263d448",
   "metadata": {},
   "source": [
    "## 1. Check GPU Availability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd9f0990",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'gppvae (Python 3.6.13)' requires the ipykernel package.\n",
      "\u001b[1;31mInstall 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: 'conda install -n gppvae ipykernel --update-deps --force-reinstall'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU Device: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è WARNING: GPU not detected! Go to Runtime ‚Üí Change runtime type ‚Üí GPU\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee1ce7c1",
   "metadata": {},
   "source": [
    "## 2. Auto-Detect Project Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24d6233f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "current_dir = os.getcwd()\n",
    "print(f\"üìç Current directory: {current_dir}\")\n",
    "\n",
    "# Task configuration\n",
    "DATA_TASK = \"task1_standard\"\n",
    "KERNEL_TYPE = \"full_rank\"\n",
    "\n",
    "if current_dir == '/content':\n",
    "    print(\"\\nüîÑ Mounting Google Drive...\")\n",
    "    try:\n",
    "        from google.colab import drive\n",
    "        drive.mount('/content/drive')\n",
    "        drive_path = '/content/drive/MyDrive/gppvae'\n",
    "        if os.path.exists(drive_path):\n",
    "            PROJECT_PATH = drive_path\n",
    "            print(f\"‚úÖ Found project in Google Drive: {PROJECT_PATH}\")\n",
    "        else:\n",
    "            print(f\"‚ö†Ô∏è Project not found at: {drive_path}\")\n",
    "            PROJECT_PATH = '/content'\n",
    "    except Exception as e:\n",
    "        print(f\"Could not mount Drive: {e}\")\n",
    "        PROJECT_PATH = '/content'\n",
    "else:\n",
    "    if 'notebooks' in current_dir:\n",
    "        PROJECT_PATH = os.path.dirname(current_dir)\n",
    "    else:\n",
    "        PROJECT_PATH = current_dir\n",
    "    print(f\"üíª Using project path: {PROJECT_PATH}\")\n",
    "\n",
    "# Verify structure\n",
    "print(f\"\\nüìÅ Contents of {PROJECT_PATH}:\")\n",
    "if os.path.exists(PROJECT_PATH):\n",
    "    for item in sorted(os.listdir(PROJECT_PATH))[:10]:\n",
    "        item_path = os.path.join(PROJECT_PATH, item)\n",
    "        marker = \"üìÇ\" if os.path.isdir(item_path) else \"üìÑ\"\n",
    "        print(f\"   {marker} {item}\")\n",
    "\n",
    "# Check required files\n",
    "print(f\"\\nüîç Checking required files:\")\n",
    "data_path = os.path.join(PROJECT_PATH, f'data/coil100/coil100_{DATA_TASK}.h5')\n",
    "required = {\n",
    "    'GPPVAE code': os.path.exists(os.path.join(PROJECT_PATH, 'GPPVAE')),\n",
    "    'COIL-100 data': os.path.exists(data_path),\n",
    "}\n",
    "\n",
    "# Check for VAE runs\n",
    "vae_base_dir = os.path.join(PROJECT_PATH, f'out/vae_colab_{DATA_TASK}_standard')\n",
    "vae_run_found = False\n",
    "if os.path.exists(vae_base_dir):\n",
    "    runs = [d for d in os.listdir(vae_base_dir) if os.path.isdir(os.path.join(vae_base_dir, d))]\n",
    "    for run in runs:\n",
    "        weights_dir = os.path.join(vae_base_dir, run, 'weights')\n",
    "        if os.path.exists(weights_dir) and any(f.endswith('.pt') for f in os.listdir(weights_dir)):\n",
    "            vae_run_found = True\n",
    "            break\n",
    "required['VAE weights'] = vae_run_found\n",
    "\n",
    "for name, exists in required.items():\n",
    "    status = \"‚úÖ\" if exists else \"‚ùå\"\n",
    "    print(f\"   {status} {name}\")\n",
    "\n",
    "if not required['VAE weights']:\n",
    "    print(f\"\\n‚ö†Ô∏è No trained VAE found! Run train_vae_colab_standard.ipynb first.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "091df06d",
   "metadata": {},
   "source": [
    "## 3. Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8aeb933b",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q wandb==0.12.21 imageio==2.15.0 pyyaml\n",
    "\n",
    "import wandb\n",
    "import imageio\n",
    "import yaml\n",
    "import numpy as np\n",
    "print(\"‚úÖ All dependencies installed successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ff5892a",
   "metadata": {},
   "source": [
    "## 4. Login to W&B (Optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d977504",
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb\n",
    "wandb.login()\n",
    "\n",
    "# Or run offline:\n",
    "# import os\n",
    "# os.environ['WANDB_MODE'] = 'offline'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8794e205",
   "metadata": {},
   "source": [
    "## 5. Navigate to Project Directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78395e4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "os.chdir(PROJECT_PATH)\n",
    "print(f\"Current directory: {os.getcwd()}\")\n",
    "\n",
    "# Add to Python path\n",
    "sys.path.insert(0, os.path.join(PROJECT_PATH, 'GPPVAE/pysrc/coil100'))\n",
    "\n",
    "print(\"\\nProject structure:\")\n",
    "!ls -la"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "961a0498",
   "metadata": {},
   "source": [
    "## 6. Find VAE Weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88ccaa37",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "\n",
    "vae_base_dir = f'./out/vae_colab_{DATA_TASK}_standard'\n",
    "vae_runs = []\n",
    "\n",
    "if os.path.exists(vae_base_dir):\n",
    "    for run_dir in sorted(os.listdir(vae_base_dir), reverse=True):\n",
    "        run_path = os.path.join(vae_base_dir, run_dir)\n",
    "        cfg_path = os.path.join(run_path, 'vae.cfg.p')\n",
    "        weights_dir = os.path.join(run_path, 'weights')\n",
    "        \n",
    "        if os.path.exists(cfg_path) and os.path.exists(weights_dir):\n",
    "            weight_files = sorted([f for f in os.listdir(weights_dir) if f.endswith('.pt')])\n",
    "            if weight_files:\n",
    "                vae_runs.append({\n",
    "                    'run_dir': run_dir,\n",
    "                    'cfg_path': cfg_path,\n",
    "                    'weights_dir': weights_dir,\n",
    "                    'weight_files': weight_files\n",
    "                })\n",
    "\n",
    "if vae_runs:\n",
    "    print(f\"‚úÖ Found {len(vae_runs)} VAE training run(s):\\n\")\n",
    "    for i, run in enumerate(vae_runs[:3], 1):\n",
    "        print(f\"Run {i}: {run['run_dir']}\")\n",
    "        vae_cfg = pickle.load(open(run['cfg_path'], 'rb'))\n",
    "        print(f\"   Config: zdim={vae_cfg.get('zdim', 'N/A')}, nf={vae_cfg.get('nf', 'N/A')}\")\n",
    "        print(f\"   Checkpoints: {len(run['weight_files'])} files\")\n",
    "        print(f\"   Latest: {run['weight_files'][-1]}\")\n",
    "    \n",
    "    # Recommend latest\n",
    "    latest = vae_runs[0]\n",
    "    print(f\"\\nüí° Recommendation: Use {latest['run_dir']}\")\n",
    "    print(f\"   VAE_CFG = '{latest['cfg_path']}'\")\n",
    "    print(f\"   VAE_WEIGHTS = '{os.path.join(latest['weights_dir'], latest['weight_files'][-1])}'\")\n",
    "else:\n",
    "    print(\"‚ùå No VAE runs found! Train VAE first.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1f744ff",
   "metadata": {},
   "source": [
    "## 7. Configure Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "889c48bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "\n",
    "# ============================================================================\n",
    "# UPDATE THESE PATHS based on Cell 6 output!\n",
    "# ============================================================================\n",
    "VAE_CFG = './out/vae_colab_task1_standard_standard/YYYYMMDD_HHMMSS/vae.cfg.p'  # UPDATE THIS\n",
    "VAE_WEIGHTS = './out/vae_colab_task1_standard_standard/YYYYMMDD_HHMMSS/weights/weights.00499.pt'  # UPDATE THIS\n",
    "\n",
    "CONFIG = {\n",
    "    # Data\n",
    "    'data': f'./data/coil100/coil100_{DATA_TASK}.h5',\n",
    "    'outdir': f'./out/gppvae_coil100_{KERNEL_TYPE}_{DATA_TASK}/{timestamp}',\n",
    "    \n",
    "    # VAE (pre-trained)\n",
    "    'vae_cfg': VAE_CFG,\n",
    "    'vae_weights': VAE_WEIGHTS,\n",
    "    \n",
    "    # Training (increased for early stopping)\n",
    "    'epochs': 1200,\n",
    "    'batch_size': 64,\n",
    "    'vae_lr': 0.001,\n",
    "    'gp_lr': 0.001,\n",
    "    'xdim': 64,  # Object embedding dimension\n",
    "    \n",
    "    # Kernel - FullRank\n",
    "    'view_kernel': KERNEL_TYPE,\n",
    "    'kernel_kwargs': {},  # FullRank has no hyperparameters\n",
    "    \n",
    "    # Logging\n",
    "    'epoch_cb': 100,\n",
    "    'use_wandb': True,\n",
    "    'wandb_project': 'gppvae-coil100',\n",
    "    'wandb_run_name': f'gppvae_{KERNEL_TYPE}_{DATA_TASK}_{timestamp}',\n",
    "    'seed': 0,\n",
    "}\n",
    "\n",
    "print(\"GP-VAE Training Configuration:\")\n",
    "print(\"=\" * 60)\n",
    "for key, value in CONFIG.items():\n",
    "    print(f\"  {key:20s}: {value}\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "if not os.path.exists(CONFIG['vae_weights']):\n",
    "    print(f\"\\n‚ö†Ô∏è Update VAE_CFG and VAE_WEIGHTS paths based on Cell 6 output!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8eb0ee4c",
   "metadata": {},
   "source": [
    "## 8. Import Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cad9d50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMPORTANT: Add coil100 to path FIRST before importing anything\n",
    "# This ensures coil100's data_parser is used, not faceplace's\n",
    "import sys\n",
    "import os\n",
    "\n",
    "coil100_path = os.path.join(PROJECT_PATH, 'GPPVAE/pysrc/coil100')\n",
    "if coil100_path not in sys.path:\n",
    "    sys.path.insert(0, coil100_path)\n",
    "\n",
    "os.chdir(coil100_path)\n",
    "print(f\"Working directory: {os.getcwd()}\")\n",
    "\n",
    "import matplotlib\n",
    "matplotlib.use('Agg')\n",
    "\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import DataLoader\n",
    "from vae import FaceVAE\n",
    "from vmod import Vmodel\n",
    "from gp import GP\n",
    "import numpy as np\n",
    "import logging\n",
    "import pylab as pl\n",
    "from utils import smartSum, smartAppendDict, smartAppend, export_scripts\n",
    "from callbacks import callback_gppvae\n",
    "import pickle\n",
    "import time\n",
    "import wandb\n",
    "\n",
    "# COIL-100 data parser (explicitly import from coil100, not faceplace)\n",
    "from data_parser import COIL100Dataset, get_n_views, get_num_objects\n",
    "\n",
    "# Verify we're using the right data_parser\n",
    "import data_parser\n",
    "print(f\"‚úÖ data_parser loaded from: {data_parser.__file__}\")\n",
    "print(\"‚úÖ All modules imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ced5a33a",
   "metadata": {},
   "source": [
    "## 9. Setup Training Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd4da2c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(PROJECT_PATH)\n",
    "\n",
    "# Create output directories\n",
    "outdir = CONFIG['outdir']\n",
    "wdir = os.path.join(outdir, \"weights\")\n",
    "fdir = os.path.join(outdir, \"plots\")\n",
    "os.makedirs(wdir, exist_ok=True)\n",
    "os.makedirs(fdir, exist_ok=True)\n",
    "\n",
    "# Device\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Logging\n",
    "log_format = \"%(asctime)s %(message)s\"\n",
    "logging.basicConfig(level=logging.INFO, format=log_format, datefmt=\"%m/%d %I:%M:%S %p\")\n",
    "fh = logging.FileHandler(os.path.join(outdir, \"log.txt\"))\n",
    "fh.setFormatter(logging.Formatter(log_format))\n",
    "logging.getLogger().addHandler(fh)\n",
    "\n",
    "# Copy scripts\n",
    "export_scripts(os.path.join(outdir, \"scripts\"))\n",
    "\n",
    "print(f\"‚úÖ Output directory: {outdir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76a36688",
   "metadata": {},
   "source": [
    "## 10. Initialize Models and Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cef0465",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(CONFIG['seed'])\n",
    "\n",
    "# Initialize W&B\n",
    "if CONFIG['use_wandb']:\n",
    "    wandb.init(\n",
    "        project=CONFIG['wandb_project'],\n",
    "        name=CONFIG['wandb_run_name'],\n",
    "        config=CONFIG\n",
    "    )\n",
    "\n",
    "# Load VAE config\n",
    "vae_cfg = pickle.load(open(CONFIG['vae_cfg'], \"rb\"))\n",
    "print(f\"VAE config: {vae_cfg}\")\n",
    "\n",
    "# Load pre-trained VAE\n",
    "print(\"\\nLoading pre-trained VAE...\")\n",
    "vae = FaceVAE(**vae_cfg).to(device)\n",
    "vae_state = torch.load(CONFIG['vae_weights'], map_location=device)\n",
    "vae.load_state_dict(vae_state)\n",
    "print(f\"‚úÖ VAE loaded from {CONFIG['vae_weights']}\")\n",
    "\n",
    "# Load COIL-100 data\n",
    "print(\"\\nLoading COIL-100 data...\")\n",
    "train_data = COIL100Dataset(CONFIG['data'], split='train', use_angle_encoding=False)\n",
    "val_data = COIL100Dataset(CONFIG['data'], split='val', use_angle_encoding=False)\n",
    "\n",
    "train_queue = DataLoader(train_data, batch_size=CONFIG['batch_size'], shuffle=True)\n",
    "val_queue = DataLoader(val_data, batch_size=CONFIG['batch_size'], shuffle=False)\n",
    "\n",
    "# Get P and Q\n",
    "# IMPORTANT: Use get_num_objects to get correct P (includes all objects from all splits)\n",
    "# This ensures embedding indices don't go out of bounds\n",
    "P = get_num_objects(CONFIG['data'])  # 100 for COIL-100\n",
    "Q = get_n_views()  # 18\n",
    "print(f\"\\nP (objects): {P}\")\n",
    "print(f\"Q (views): {Q}\")\n",
    "\n",
    "# Create object and view tensors (Did and Rid are 1D tensors)\n",
    "Dt = Variable(train_data.Did.long(), requires_grad=False).to(device)\n",
    "Dv = Variable(val_data.Did.long(), requires_grad=False).to(device)\n",
    "Wt = Variable(train_data.Rid.long(), requires_grad=False).to(device)\n",
    "Wv = Variable(val_data.Rid.long(), requires_grad=False).to(device)\n",
    "\n",
    "print(f\"\\nTrain views: {torch.unique(Wt).cpu().numpy()}\")\n",
    "print(f\"Val views: {torch.unique(Wv).cpu().numpy()}\")\n",
    "\n",
    "# Initialize Vmodel with FullRank kernel\n",
    "print(f\"\\nüî¨ Initializing Vmodel with '{KERNEL_TYPE}' kernel...\")\n",
    "vm = Vmodel(\n",
    "    P=P, Q=Q, p=CONFIG['xdim'],\n",
    "    view_kernel=CONFIG['view_kernel'],\n",
    "    **CONFIG['kernel_kwargs']\n",
    ").to(device)\n",
    "\n",
    "gp = GP(n_rand_effs=1).to(device)\n",
    "\n",
    "# Combine GP parameters\n",
    "gp_params = nn.ParameterList()\n",
    "gp_params.extend(vm.parameters())\n",
    "gp_params.extend(gp.parameters())\n",
    "\n",
    "print(f\"\\n‚úÖ Models initialized:\")\n",
    "print(f\"   VAE parameters: {sum(p.numel() for p in vae.parameters()):,}\")\n",
    "print(f\"   Vmodel parameters: {sum(p.numel() for p in vm.parameters()):,}\")\n",
    "print(f\"   GP parameters: {sum(p.numel() for p in gp.parameters()):,}\")\n",
    "\n",
    "# Optimizers\n",
    "vae_optimizer = optim.Adam(vae.parameters(), lr=CONFIG['vae_lr'])\n",
    "gp_optimizer = optim.Adam(gp_params, lr=CONFIG['gp_lr'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27985a0b",
   "metadata": {},
   "source": [
    "## 11. Define Training Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28fd11f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_Y(vae, train_queue):\n",
    "    \"\"\"Encode all training images to get latent codes\"\"\"\n",
    "    vae.eval()\n",
    "    with torch.no_grad():\n",
    "        n = train_queue.dataset.Y.shape[0]\n",
    "        Zm = Variable(torch.zeros(n, vae_cfg[\"zdim\"]), requires_grad=False).to(device)\n",
    "        Zs = Variable(torch.zeros(n, vae_cfg[\"zdim\"]), requires_grad=False).to(device)\n",
    "        \n",
    "        for batch_i, data in enumerate(train_queue):\n",
    "            y = data[0].to(device)\n",
    "            idxs = data[-1].to(device)\n",
    "            zm, zs = vae.encode(y)\n",
    "            Zm[idxs], Zs[idxs] = zm.detach(), zs.detach()\n",
    "    return Zm, Zs\n",
    "\n",
    "\n",
    "def eval_step(vae, gp, vm, val_queue, Zm, Vt, Vv, Wv):\n",
    "    \"\"\"Evaluation step\"\"\"\n",
    "    rv = {}\n",
    "    with torch.no_grad():\n",
    "        _X = vm.x().data.cpu().numpy()\n",
    "        _W = vm.v().data.cpu().numpy()\n",
    "        covs = {\"XX\": np.dot(_X, _X.T), \"WW\": np.dot(_W, _W.T)}\n",
    "        rv[\"vars\"] = gp.get_vs().data.cpu().numpy()\n",
    "        \n",
    "        # Out-of-sample prediction\n",
    "        vs = gp.get_vs()\n",
    "        U, UBi, _ = gp.U_UBi_Shb([Vt], vs)\n",
    "        Kiz = gp.solve(Zm, U, UBi, vs)\n",
    "        Zo = vs[0] * Vv.mm(Vt.transpose(0, 1).mm(Kiz))\n",
    "        \n",
    "        mse_out = Variable(torch.zeros(Vv.shape[0], 1), requires_grad=False).to(device)\n",
    "        mse_val = Variable(torch.zeros(Vv.shape[0], 1), requires_grad=False).to(device)\n",
    "        \n",
    "        all_Yv, all_Yr, all_Yo = [], [], []\n",
    "        \n",
    "        for batch_i, data in enumerate(val_queue):\n",
    "            idxs = data[-1].to(device)\n",
    "            Yv = data[0].to(device)\n",
    "            Zv = vae.encode(Yv)[0].detach()\n",
    "            Yr = vae.decode(Zv)\n",
    "            Yo = vae.decode(Zo[idxs])\n",
    "            mse_out[idxs] = ((Yv - Yo) ** 2).view(Yv.shape[0], -1).mean(1)[:, None].detach()\n",
    "            mse_val[idxs] = ((Yv - Yr) ** 2).view(Yv.shape[0], -1).mean(1)[:, None].detach()\n",
    "            \n",
    "            all_Yv.append(Yv.data.cpu().numpy().transpose(0, 2, 3, 1))\n",
    "            all_Yr.append(Yr.data.cpu().numpy().transpose(0, 2, 3, 1))\n",
    "            all_Yo.append(Yo.data.cpu().numpy().transpose(0, 2, 3, 1))\n",
    "        \n",
    "        all_Yv = np.concatenate(all_Yv, axis=0)\n",
    "        all_Yr = np.concatenate(all_Yr, axis=0)\n",
    "        all_Yo = np.concatenate(all_Yo, axis=0)\n",
    "        \n",
    "        n_total = all_Yv.shape[0]\n",
    "        sample_stride = max(1, n_total // 24)\n",
    "        sample_indices = np.arange(0, n_total, sample_stride)[:24]\n",
    "        \n",
    "        imgs = {\n",
    "            \"Yv\": all_Yv[sample_indices],\n",
    "            \"Yr\": all_Yr[sample_indices],\n",
    "            \"Yo\": all_Yo[sample_indices]\n",
    "        }\n",
    "        \n",
    "        rv[\"mse_out\"] = float(mse_out.data.mean().cpu())\n",
    "        rv[\"mse_val\"] = float(mse_val.data.mean().cpu())\n",
    "    \n",
    "    return rv, imgs, covs\n",
    "\n",
    "\n",
    "def backprop_and_update(vae, gp, vm, train_queue, Dt, Wt, Eps, Zb, Vbs, vbs, vae_optimizer, gp_optimizer):\n",
    "    \"\"\"Joint optimization step\"\"\"\n",
    "    rv = {}\n",
    "    vae_optimizer.zero_grad()\n",
    "    gp_optimizer.zero_grad()\n",
    "    vae.train()\n",
    "    gp.train()\n",
    "    vm.train()\n",
    "    \n",
    "    for batch_i, data in enumerate(train_queue):\n",
    "        y = data[0].to(device)\n",
    "        eps = Eps[data[-1]]\n",
    "        _d = Dt[data[-1]]\n",
    "        _w = Wt[data[-1]]\n",
    "        _Zb = Zb[data[-1]]\n",
    "        _Vbs = [Vbs[0][data[-1]]]\n",
    "        \n",
    "        zm, zs = vae.encode(y)\n",
    "        z = zm + zs * eps\n",
    "        yr = vae.decode(z)\n",
    "        recon_term, mse = vae.nll(y, yr)\n",
    "        \n",
    "        _Vs = [vm(_d, _w)]\n",
    "        gp_nll_fo = gp.taylor_expansion(z, _Vs, _Zb, _Vbs, vbs) / vae.K\n",
    "        pen_term = -0.5 * zs.sum(1)[:, None] / vae.K\n",
    "        \n",
    "        loss = (recon_term + gp_nll_fo + pen_term).sum()\n",
    "        loss.backward()\n",
    "        \n",
    "        _n = train_queue.dataset.Y.shape[0]\n",
    "        smartSum(rv, \"mse\", float(mse.data.sum().cpu()) / _n)\n",
    "        smartSum(rv, \"recon_term\", float(recon_term.data.sum().cpu()) / _n)\n",
    "        smartSum(rv, \"pen_term\", float(pen_term.data.sum().cpu()) / _n)\n",
    "    \n",
    "    vae_optimizer.step()\n",
    "    gp_optimizer.step()\n",
    "    \n",
    "    return rv\n",
    "\n",
    "print(\"‚úÖ Training functions defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a96667b2",
   "metadata": {},
   "source": [
    "## 12. Train GP-VAE üöÄ (with Early Stopping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7c69d73",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "history = {}\n",
    "start_time = time.time()\n",
    "\n",
    "# -----------------------------\n",
    "# Early stopping configuration\n",
    "# -----------------------------\n",
    "early_stop_patience = 150        # epochs without improvement\n",
    "early_stop_min_delta = 1e-4      # minimum improvement threshold\n",
    "\n",
    "best_mse_out = float(\"inf\")\n",
    "best_epoch = -1\n",
    "no_improve_epochs = 0\n",
    "\n",
    "print(f\"üöÄ Training GP-VAE with {KERNEL_TYPE} kernel for up to {CONFIG['epochs']} epochs...\")\n",
    "print(f\"üõë Early stopping patience = {early_stop_patience}\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "for epoch in range(CONFIG['epochs']):\n",
    "    epoch_start = time.time()\n",
    "\n",
    "    # -------- Encode training data --------\n",
    "    Zm, Zs = encode_Y(vae, train_queue)\n",
    "    Eps = Variable(torch.randn(*Zs.shape), requires_grad=False).to(device)\n",
    "    Z = Zm + Eps * Zs\n",
    "\n",
    "    # -------- Precompute V --------\n",
    "    Vt = vm(Dt, Wt).detach()\n",
    "    Vv = vm(Dv, Wv).detach()\n",
    "\n",
    "    # -------- Validation step --------\n",
    "    rv_eval, imgs, covs = eval_step(\n",
    "        vae, gp, vm, val_queue, Zm, Vt, Vv, Wv\n",
    "    )\n",
    "\n",
    "    # -------- GP Taylor expansion --------\n",
    "    Zb, Vbs, vbs, gp_nll = gp.taylor_coeff(Z, [Vt])\n",
    "    rv_eval[\"gp_nll\"] = float(gp_nll.data.mean().cpu()) / vae.K\n",
    "\n",
    "    # -------- Backprop --------\n",
    "    rv_back = backprop_and_update(\n",
    "        vae, gp, vm, train_queue, Dt, Wt,\n",
    "        Eps, Zb, Vbs, vbs,\n",
    "        vae_optimizer, gp_optimizer\n",
    "    )\n",
    "\n",
    "    rv_back[\"loss\"] = (\n",
    "        rv_back[\"recon_term\"] +\n",
    "        rv_eval[\"gp_nll\"] +\n",
    "        rv_back[\"pen_term\"]\n",
    "    )\n",
    "\n",
    "    # -------- Logging --------\n",
    "    smartAppendDict(history, rv_eval)\n",
    "    smartAppendDict(history, rv_back)\n",
    "    smartAppend(history, \"vs\", gp.get_vs().data.cpu().numpy())\n",
    "\n",
    "    vs = gp.get_vs().data.cpu().numpy()\n",
    "    variance_ratio = vs[0] / (vs[0] + vs[1])\n",
    "\n",
    "    current_mse_out = rv_eval[\"mse_out\"]\n",
    "\n",
    "    # -------- Early stopping check --------\n",
    "    if current_mse_out < best_mse_out - early_stop_min_delta:\n",
    "        best_mse_out = current_mse_out\n",
    "        best_epoch = epoch\n",
    "        no_improve_epochs = 0\n",
    "\n",
    "        # Save BEST checkpoint\n",
    "        torch.save(\n",
    "            vae.state_dict(),\n",
    "            os.path.join(wdir, \"vae_weights.best.pt\")\n",
    "        )\n",
    "        torch.save(\n",
    "            {'gp_state': gp.state_dict(), 'vm_state': vm.state_dict()},\n",
    "            os.path.join(wdir, \"gp_weights.best.pt\")\n",
    "        )\n",
    "\n",
    "    else:\n",
    "        no_improve_epochs += 1\n",
    "\n",
    "    # -------- Console output --------\n",
    "    if epoch % 5 == 0 or epoch == CONFIG['epochs'] - 1:\n",
    "        print(\n",
    "            f\"Epoch {epoch:4d} | \"\n",
    "            f\"MSE train: {rv_back['mse']:.6f} | \"\n",
    "            f\"MSE out: {current_mse_out:.6f} | \"\n",
    "            f\"GP NLL: {rv_eval['gp_nll']:.4f} | \"\n",
    "            f\"v‚ÇÄ/(v‚ÇÄ+v‚ÇÅ): {variance_ratio:.3f}\"\n",
    "        )\n",
    "\n",
    "    # -------- wandb --------\n",
    "    if CONFIG['use_wandb']:\n",
    "        log_dict = {\n",
    "            \"epoch\": epoch,\n",
    "            \"mse_train\": rv_back[\"mse\"],\n",
    "            \"mse_out\": current_mse_out,\n",
    "            \"gp_nll\": rv_eval[\"gp_nll\"],\n",
    "            \"variance_ratio\": variance_ratio,\n",
    "            \"best_mse_out\": best_mse_out,\n",
    "            \"no_improve_epochs\": no_improve_epochs,\n",
    "        }\n",
    "        wandb.log(log_dict)\n",
    "\n",
    "    # -------- Periodic checkpoint + plots --------\n",
    "    if epoch % CONFIG['epoch_cb'] == 0 or epoch == CONFIG['epochs'] - 1:\n",
    "        torch.save(\n",
    "            vae.state_dict(),\n",
    "            os.path.join(wdir, f\"vae_weights.{epoch:05d}.pt\")\n",
    "        )\n",
    "        torch.save(\n",
    "            {'gp_state': gp.state_dict(), 'vm_state': vm.state_dict()},\n",
    "            os.path.join(wdir, f\"gp_weights.{epoch:05d}.pt\")\n",
    "        )\n",
    "        ffile = os.path.join(fdir, f\"plot.{epoch:05d}.png\")\n",
    "        callback_gppvae(epoch, history, covs, imgs, ffile)\n",
    "        if CONFIG['use_wandb']:\n",
    "            wandb.log({\"reconstructions\": wandb.Image(ffile)})\n",
    "        print(\"  ‚úì Checkpoint saved\")\n",
    "\n",
    "    # -------- Stop condition --------\n",
    "    if no_improve_epochs >= early_stop_patience:\n",
    "        print(\n",
    "            f\"\\n‚èπ Early stopping triggered at epoch {epoch}\\n\"\n",
    "            f\"   Best epoch: {best_epoch}\\n\"\n",
    "            f\"   Best mse_out: {best_mse_out:.6f}\"\n",
    "        )\n",
    "        break\n",
    "\n",
    "print(\n",
    "    f\"\\n‚úÖ Training complete in {(time.time()-start_time)/60:.1f} min\\n\"\n",
    "    f\"   Best epoch: {best_epoch}\\n\"\n",
    "    f\"   Best mse_out: {best_mse_out:.6f}\"\n",
    ")\n",
    "\n",
    "if CONFIG['use_wandb']:\n",
    "    wandb.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf211175",
   "metadata": {},
   "source": [
    "## 13. View Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4280b9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image, display\n",
    "import glob\n",
    "\n",
    "plot_files = sorted(glob.glob(os.path.join(fdir, \"*.png\")))\n",
    "if plot_files:\n",
    "    print(f\"Latest plot: {plot_files[-1]}\")\n",
    "    display(Image(filename=plot_files[-1]))\n",
    "else:\n",
    "    print(\"No plots generated yet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b07aa48",
   "metadata": {},
   "source": [
    "## 14. Download Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3971e91",
   "metadata": {},
   "outputs": [],
   "source": [
    "!zip -r /content/gppvae_fullrank_output.zip {CONFIG['outdir']}\n",
    "\n",
    "from google.colab import files\n",
    "files.download('/content/gppvae_fullrank_output.zip')\n",
    "print(\"‚úÖ Download started!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gppvae",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
