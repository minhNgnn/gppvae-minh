{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2f0fc64f",
   "metadata": {},
   "source": [
    "# Task 2 (Interpolation) Analysis - Kernel Comparison\n",
    "\n",
    "This notebook analyzes the performance of different GP kernels on **Task 2 (Interpolation)**:\n",
    "\n",
    "## Task Description\n",
    "- **Task 2 (Interpolation)**: Predict **intermediate views** within the training range\n",
    "- **Train Views**: Even-indexed views (0Â°, 40Â°, 80Â°, 120Â°, 160Â°, 200Â°, 240Â°, 280Â°, 320Â°)\n",
    "- **Val/Test Views**: Odd-indexed views (20Â°, 60Â°, 100Â°, 140Â°, 180Â°, 220Â°, 260Â°, 300Â°, 340Â°)\n",
    "- **Goal**: Test in-range generalization via GP interpolation\n",
    "\n",
    "## Research Question\n",
    "> \"Does imposing smoothness via structured kernels improve performance when test views lie **within** the training range?\"\n",
    "\n",
    "## Kernels Compared\n",
    "1. **Full Rank**: Free-form learnable covariance (QÃ—Q parameters)\n",
    "2. **Periodic**: Standard periodic kernel with learned lengthscale\n",
    "3. **SM Wrapped**: Spectral Mixture with wrapped lag distance\n",
    "4. **SM Free**: Spectral Mixture (unwrapped)\n",
    "\n",
    "## Analysis (Extended for Interpolation)\n",
    "- Load best checkpoints from each kernel (5 seeds each)\n",
    "- Evaluate on test set (intermediate views)\n",
    "- **Per-View MSE**: Analyze how well each kernel interpolates at different angles\n",
    "- **Distance-to-Nearest-Train Analysis**: MSE vs distance to nearest training view\n",
    "- Visualize kernel matrices and reconstructions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7169ed19",
   "metadata": {},
   "source": [
    "## 1. Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9e0cdab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import glob\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "from matplotlib.patches import Patch\n",
    "import seaborn as sns\n",
    "\n",
    "# Set style\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "sns.set_palette('husl')\n",
    "\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fcd0bcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "current_dir = os.getcwd()\n",
    "print(f\"ğŸ“ Current directory: {current_dir}\")\n",
    "\n",
    "if current_dir == '/content':\n",
    "    print(\"\\nğŸ”„ Mounting Google Drive...\")\n",
    "    try:\n",
    "        from google.colab import drive\n",
    "        drive.mount('/content/drive')\n",
    "        drive_path = '/content/drive/MyDrive/gppvae'\n",
    "        if os.path.exists(drive_path):\n",
    "            PROJECT_PATH = drive_path\n",
    "            print(f\"âœ… Found project in Google Drive: {PROJECT_PATH}\")\n",
    "        else:\n",
    "            print(f\"âš ï¸ Project not found at: {drive_path}\")\n",
    "            PROJECT_PATH = '/content'\n",
    "    except Exception as e:\n",
    "        print(f\"Could not mount Drive: {e}\")\n",
    "        PROJECT_PATH = '/content'\n",
    "else:\n",
    "    if 'notebooks' in current_dir:\n",
    "        PROJECT_PATH = os.path.dirname(os.path.dirname(current_dir))\n",
    "    else:\n",
    "        PROJECT_PATH = current_dir\n",
    "    print(f\"ğŸ’» Using project path: {PROJECT_PATH}\")\n",
    "\n",
    "# Add code paths\n",
    "coil100_path = os.path.join(PROJECT_PATH, 'GPPVAE/pysrc/coil100')\n",
    "sys.path.insert(0, coil100_path)\n",
    "\n",
    "# Change to project root\n",
    "os.chdir(PROJECT_PATH)\n",
    "print(f\"Working directory: {os.getcwd()}\")\n",
    "print(f\"Code path added: {coil100_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96b83ebe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import models\n",
    "from vae import FaceVAE\n",
    "from vmod import Vmodel\n",
    "from gp import GP\n",
    "from data_parser import COIL100Dataset, get_n_views, get_num_objects\n",
    "\n",
    "print(\"âœ… All modules imported\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54ab0afe",
   "metadata": {},
   "source": [
    "## 2. Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5541a25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task 2 (Interpolation) configuration\n",
    "CONFIG = {\n",
    "    'task': 'task2_interpolation',\n",
    "    'data_path': './data/coil100/coil100_task2_interpolation.h5',\n",
    "    'batch_size': 64,\n",
    "    'xdim': 64,  # Object embedding dimension\n",
    "}\n",
    "\n",
    "# Auto-detect results folder location\n",
    "if os.path.exists('./GPPVAE/results'):\n",
    "    CONFIG['results_base'] = './GPPVAE/results'\n",
    "    print(f\"âœ… Found results at: {os.path.abspath(CONFIG['results_base'])}\")\n",
    "elif os.path.exists('./results'):\n",
    "    CONFIG['results_base'] = './results'\n",
    "    print(f\"âœ… Found results at: {os.path.abspath(CONFIG['results_base'])}\")\n",
    "else:\n",
    "    CONFIG['results_base'] = './results'\n",
    "    print(f\"âš ï¸ Results folder not found, using default: {CONFIG['results_base']}\")\n",
    "\n",
    "# Auto-detect data folder location\n",
    "if os.path.exists('./GPPVAE/data/coil100/coil100_task2_interpolation.h5'):\n",
    "    CONFIG['data_path'] = './GPPVAE/data/coil100/coil100_task2_interpolation.h5'\n",
    "    print(f\"âœ… Found data at: {os.path.abspath(CONFIG['data_path'])}\")\n",
    "elif os.path.exists('./data/coil100/coil100_task2_interpolation.h5'):\n",
    "    CONFIG['data_path'] = './data/coil100/coil100_task2_interpolation.h5'\n",
    "    print(f\"âœ… Found data at: {os.path.abspath(CONFIG['data_path'])}\")\n",
    "else:\n",
    "    CONFIG['data_path'] = './data/coil100/coil100_task2_interpolation.h5'\n",
    "    print(f\"âš ï¸ Data file not found, using default: {CONFIG['data_path']}\")\n",
    "\n",
    "# View configuration for Task 2 (Interpolation)\n",
    "# Train: even views (0, 2, 4, ..., 16) = 0Â°, 40Â°, 80Â°, ..., 320Â°\n",
    "# Test: odd views (1, 3, 5, ..., 17) = 20Â°, 60Â°, 100Â°, ..., 340Â°\n",
    "TRAIN_VIEW_INDICES = [0, 2, 4, 6, 8, 10, 12, 14, 16]  # Even indices\n",
    "TEST_VIEW_INDICES = [1, 3, 5, 7, 9, 11, 13, 15, 17]   # Odd indices (interpolation targets)\n",
    "VIEW_ANGLES = {i: i * 20 for i in range(18)}  # View index to angle mapping\n",
    "\n",
    "print(f\"\\nğŸ“Š Interpolation Task Setup:\")\n",
    "print(f\"   Train views: {[VIEW_ANGLES[v] for v in TRAIN_VIEW_INDICES]}Â°\")\n",
    "print(f\"   Test views:  {[VIEW_ANGLES[v] for v in TEST_VIEW_INDICES]}Â°\")\n",
    "\n",
    "# Kernel configurations (must match training)\n",
    "KERNEL_CONFIGS = {\n",
    "    'fullrank': {\n",
    "        'folder': 'task2_fullrank',\n",
    "        'view_kernel': 'full_rank',\n",
    "        'kernel_kwargs': {},\n",
    "        'display_name': 'Full Rank',\n",
    "        'color': '#e74c3c',\n",
    "    },\n",
    "    'periodic': {\n",
    "        'folder': 'task2_periodic',\n",
    "        'view_kernel': 'periodic',\n",
    "        'kernel_kwargs': {'period': 360.0, 'lengthscale': 1.0, 'variance': 1.0},\n",
    "        'display_name': 'Periodic',\n",
    "        'color': '#3498db',\n",
    "    },\n",
    "    'sm_wrapped': {\n",
    "        'folder': 'task2_sm_wrapped',\n",
    "        'view_kernel': 'sm_circle',\n",
    "        'kernel_kwargs': {'freq_init': [1/360.0, 1/40.0], 'weight_init':[0.5, 0.5], 'length_init':[90,30]},\n",
    "        'display_name': 'SM (Wrapped)',\n",
    "        'color': '#2ecc71',\n",
    "    },\n",
    "    'sm_free': {\n",
    "        'folder': 'task2_sm_free',\n",
    "        'view_kernel': 'sm_circle',\n",
    "        'kernel_kwargs': {'freq_init': [1/360.0, 1/40.0], 'weight_init':[0.5, 0.5], 'use_angle_input': True},\n",
    "        'display_name': 'SM (Free)',\n",
    "        'color': '#9b59b6',\n",
    "    },\n",
    "}\n",
    "\n",
    "# Device\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52fd03c6",
   "metadata": {},
   "source": [
    "## 3. Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d35e5c3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load datasets\n",
    "train_data = COIL100Dataset(CONFIG['data_path'], split='train', use_angle_encoding=False)\n",
    "val_data = COIL100Dataset(CONFIG['data_path'], split='val', use_angle_encoding=False)\n",
    "test_data = COIL100Dataset(CONFIG['data_path'], split='test', use_angle_encoding=False)\n",
    "\n",
    "train_queue = DataLoader(train_data, batch_size=CONFIG['batch_size'], shuffle=False)\n",
    "val_queue = DataLoader(val_data, batch_size=CONFIG['batch_size'], shuffle=False)\n",
    "test_queue = DataLoader(test_data, batch_size=CONFIG['batch_size'], shuffle=False)\n",
    "\n",
    "# Get dimensions\n",
    "P = get_num_objects(CONFIG['data_path'])  # Number of objects\n",
    "Q = get_n_views()  # Number of views (18)\n",
    "\n",
    "print(f\"\\nğŸ“Š Task 2 (Interpolation) Dataset:\")\n",
    "print(f\"   Objects (P): {P}\")\n",
    "print(f\"   Views (Q): {Q}\")\n",
    "print(f\"   Train: {len(train_data)} samples\")\n",
    "print(f\"   Val: {len(val_data)} samples\")\n",
    "print(f\"   Test: {len(test_data)} samples\")\n",
    "\n",
    "# Analyze view distribution\n",
    "train_views = set(train_data.Rid.numpy())\n",
    "test_views = set(test_data.Rid.numpy())\n",
    "print(f\"\\n   Train view indices: {sorted(train_views)}\")\n",
    "print(f\"   Test view indices: {sorted(test_views)}\")\n",
    "print(f\"   Train angles: {[VIEW_ANGLES.get(int(v), v*20) for v in sorted(train_views)]}Â°\")\n",
    "print(f\"   Test angles: {[VIEW_ANGLES.get(int(v), v*20) for v in sorted(test_views)]}Â°\")\n",
    "\n",
    "# Create tensors for indices\n",
    "Dt = Variable(train_data.Did.long(), requires_grad=False).to(device)\n",
    "Wt = Variable(train_data.Rid.long(), requires_grad=False).to(device)\n",
    "Dtest = Variable(test_data.Did.long(), requires_grad=False).to(device)\n",
    "Wtest = Variable(test_data.Rid.long(), requires_grad=False).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62b928ed",
   "metadata": {},
   "source": [
    "## 4. Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77a8f0b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_run_folders(results_base, kernel_folder):\n",
    "    \"\"\"Find all run folders (seeds) for a kernel.\"\"\"\n",
    "    kernel_path = os.path.join(results_base, kernel_folder)\n",
    "    if not os.path.exists(kernel_path):\n",
    "        print(f\"âš ï¸ Kernel folder not found: {kernel_path}\")\n",
    "        return []\n",
    "    \n",
    "    runs = sorted([d for d in os.listdir(kernel_path) \n",
    "                   if os.path.isdir(os.path.join(kernel_path, d))])\n",
    "    return [os.path.join(kernel_path, r) for r in runs]\n",
    "\n",
    "\n",
    "def load_vae_config(run_folder=None):\n",
    "    \"\"\"Load VAE config from GPPVAE folder structure or use default.\"\"\"\n",
    "    possible_paths = [\n",
    "        './GPPVAE/pysrc/coil100/vae.cfg.p',\n",
    "        './out/vae/vae.cfg.p',\n",
    "        './out/vae_colab/*/vae.cfg.p',\n",
    "    ]\n",
    "    \n",
    "    for path_pattern in possible_paths:\n",
    "        if '*' in path_pattern:\n",
    "            matches = glob.glob(path_pattern)\n",
    "            if matches:\n",
    "                try:\n",
    "                    with open(matches[0], 'rb') as f:\n",
    "                        vae_cfg = pickle.load(f)\n",
    "                    return vae_cfg\n",
    "                except:\n",
    "                    continue\n",
    "        elif os.path.exists(path_pattern):\n",
    "            try:\n",
    "                with open(path_pattern, 'rb') as f:\n",
    "                    vae_cfg = pickle.load(f)\n",
    "                return vae_cfg\n",
    "            except:\n",
    "                continue\n",
    "    \n",
    "    # Default VAE config for COIL-100\n",
    "    vae_cfg = {\n",
    "        'img_size': 128,\n",
    "        'nf': 32,\n",
    "        'zdim': 256,\n",
    "        'steps': 5,\n",
    "        'colors': 3,\n",
    "        'act': 'elu',\n",
    "        'vy': 0.001\n",
    "    }\n",
    "    return vae_cfg\n",
    "\n",
    "\n",
    "def load_models(run_folder, kernel_config, P, Q, xdim, device):\n",
    "    \"\"\"Load VAE, Vmodel, and GP from a run folder's best checkpoint.\"\"\"\n",
    "    weights_dir = os.path.join(run_folder, 'weights')\n",
    "    gp_weights_path = os.path.join(weights_dir, 'gp_weights.best.pt')\n",
    "    vae_weights_path = os.path.join(weights_dir, 'vae_weights.best.pt')\n",
    "    \n",
    "    if not os.path.exists(gp_weights_path):\n",
    "        raise FileNotFoundError(f\"GP weights not found: {gp_weights_path}\")\n",
    "    if not os.path.exists(vae_weights_path):\n",
    "        raise FileNotFoundError(f\"VAE weights not found: {vae_weights_path}\")\n",
    "    \n",
    "    # Load VAE\n",
    "    vae_cfg = load_vae_config(run_folder)\n",
    "    vae = FaceVAE(**vae_cfg).to(device)\n",
    "    vae.load_state_dict(torch.load(vae_weights_path, map_location=device))\n",
    "    vae.eval()\n",
    "    \n",
    "    # Load Vmodel and GP\n",
    "    vm = Vmodel(\n",
    "        P=P, Q=Q, p=xdim,\n",
    "        view_kernel=kernel_config['view_kernel'],\n",
    "        **kernel_config['kernel_kwargs']\n",
    "    ).to(device)\n",
    "    gp = GP(n_rand_effs=1).to(device)\n",
    "    \n",
    "    checkpoint = torch.load(gp_weights_path, map_location=device)\n",
    "    gp.load_state_dict(checkpoint['gp_state'])\n",
    "    vm.load_state_dict(checkpoint['vm_state'])\n",
    "    \n",
    "    vm.eval()\n",
    "    gp.eval()\n",
    "    \n",
    "    return vae, vm, gp\n",
    "\n",
    "\n",
    "def encode_dataset(vae, data_queue, device):\n",
    "    \"\"\"Encode all images in a dataset to latent space.\"\"\"\n",
    "    vae.eval()\n",
    "    n = data_queue.dataset.Y.shape[0]\n",
    "    zdim = 256\n",
    "    \n",
    "    Zm = torch.zeros(n, zdim).to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for data in data_queue:\n",
    "            y = data[0].to(device)\n",
    "            idxs = data[-1].to(device)\n",
    "            zm, _ = vae.encode(y)\n",
    "            Zm[idxs] = zm.detach()\n",
    "    \n",
    "    return Zm\n",
    "\n",
    "\n",
    "def evaluate_on_test_with_per_view(vae, vm, gp, train_queue, test_queue, \n",
    "                                    Dt, Wt, Dtest, Wtest, device):\n",
    "    \"\"\"\n",
    "    Evaluate model on test set with per-view MSE breakdown.\n",
    "    \n",
    "    Returns:\n",
    "        mse_test: Mean MSE on test set\n",
    "        mse_per_sample: MSE for each test sample\n",
    "        mse_per_view: Dict mapping view index to list of MSEs\n",
    "    \"\"\"\n",
    "    vae.eval()\n",
    "    vm.eval()\n",
    "    gp.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        # Encode training data\n",
    "        Zm = encode_dataset(vae, train_queue, device)\n",
    "        \n",
    "        # Compute V matrices\n",
    "        Vt = vm(Dt, Wt).detach()\n",
    "        Vtest = vm(Dtest, Wtest).detach()\n",
    "        \n",
    "        # GP prediction\n",
    "        vs = gp.get_vs()\n",
    "        U, UBi, _ = gp.U_UBi_Shb([Vt], vs)\n",
    "        Kiz = gp.solve(Zm, U, UBi, vs)\n",
    "        Zo_test = vs[0] * Vtest.mm(Vt.transpose(0, 1).mm(Kiz))\n",
    "        \n",
    "        # Compute MSE per sample and per view\n",
    "        test_Rid = test_queue.dataset.Rid\n",
    "        mse_per_view = {}\n",
    "        mse_per_sample = []\n",
    "        mse_test_total = 0.0\n",
    "        \n",
    "        for data in test_queue:\n",
    "            idxs = data[-1].to(device)\n",
    "            Ytest = data[0].to(device)\n",
    "            Yo = vae.decode(Zo_test[idxs])\n",
    "            mse_batch = ((Ytest - Yo) ** 2).view(Ytest.shape[0], -1).mean(1)\n",
    "            \n",
    "            for i, idx in enumerate(data[-1]):\n",
    "                view = int(test_Rid[idx].item())\n",
    "                mse_val = mse_batch[i].item()\n",
    "                \n",
    "                if view not in mse_per_view:\n",
    "                    mse_per_view[view] = []\n",
    "                mse_per_view[view].append(mse_val)\n",
    "                mse_per_sample.append(mse_val)\n",
    "            \n",
    "            mse_test_total += mse_batch.sum().item()\n",
    "        \n",
    "        mse_test = mse_test_total / len(test_queue.dataset)\n",
    "    \n",
    "    return mse_test, np.array(mse_per_sample), mse_per_view\n",
    "\n",
    "\n",
    "def get_reconstructions(vae, vm, gp, train_queue, test_queue,\n",
    "                        Dt, Wt, Dtest, Wtest, device, n_samples=24):\n",
    "    \"\"\"Get sample reconstructions for visualization.\"\"\"\n",
    "    vae.eval()\n",
    "    vm.eval()\n",
    "    gp.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        Zm = encode_dataset(vae, train_queue, device)\n",
    "        Vt = vm(Dt, Wt).detach()\n",
    "        Vtest = vm(Dtest, Wtest).detach()\n",
    "        \n",
    "        vs = gp.get_vs()\n",
    "        U, UBi, _ = gp.U_UBi_Shb([Vt], vs)\n",
    "        Kiz = gp.solve(Zm, U, UBi, vs)\n",
    "        Zo_test = vs[0] * Vtest.mm(Vt.transpose(0, 1).mm(Kiz))\n",
    "        \n",
    "        n_total = len(test_queue.dataset)\n",
    "        sample_stride = max(1, n_total // n_samples)\n",
    "        sample_indices = list(range(0, n_total, sample_stride))[:n_samples]\n",
    "        \n",
    "        Y_orig = test_queue.dataset.Y[sample_indices].numpy().transpose(0, 2, 3, 1)\n",
    "        sample_indices_tensor = torch.tensor(sample_indices, dtype=torch.long).to(device)\n",
    "        Y_recon = vae.decode(Zo_test[sample_indices_tensor])\n",
    "        Y_recon = Y_recon.cpu().numpy().transpose(0, 2, 3, 1)\n",
    "    \n",
    "    return Y_orig, Y_recon\n",
    "\n",
    "\n",
    "def compute_distance_to_nearest_train(test_view_idx, train_view_indices):\n",
    "    \"\"\"Compute angular distance from test view to nearest training view.\"\"\"\n",
    "    test_angle = test_view_idx * 20\n",
    "    train_angles = [v * 20 for v in train_view_indices]\n",
    "    \n",
    "    min_dist = float('inf')\n",
    "    for train_angle in train_angles:\n",
    "        # Circular distance\n",
    "        dist = min(abs(test_angle - train_angle), 360 - abs(test_angle - train_angle))\n",
    "        min_dist = min(min_dist, dist)\n",
    "    return min_dist\n",
    "\n",
    "print(\"âœ… Helper functions defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59b58a43",
   "metadata": {},
   "source": [
    "## 5. Evaluate All Kernels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be44a1e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Results storage\n",
    "results = {}\n",
    "\n",
    "for kernel_name, kernel_config in KERNEL_CONFIGS.items():\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Evaluating: {kernel_config['display_name']}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    run_folders = find_run_folders(CONFIG['results_base'], kernel_config['folder'])\n",
    "    \n",
    "    if not run_folders:\n",
    "        print(f\"âš ï¸ No runs found for {kernel_name}\")\n",
    "        continue\n",
    "    \n",
    "    print(f\"Found {len(run_folders)} runs (seeds)\")\n",
    "    \n",
    "    kernel_results = {\n",
    "        'mse_per_seed': [],\n",
    "        'mse_per_sample_all': [],\n",
    "        'mse_per_view_all': [],  # Per-view breakdown for each seed\n",
    "        'kernel_matrices': [],\n",
    "        'variance_ratios': [],\n",
    "    }\n",
    "    \n",
    "    for i, run_folder in enumerate(run_folders):\n",
    "        try:\n",
    "            print(f\"  Seed {i}: {os.path.basename(run_folder)}...\", end=\" \")\n",
    "            \n",
    "            vae, vm, gp = load_models(\n",
    "                run_folder, kernel_config, P, Q, \n",
    "                CONFIG['xdim'], device\n",
    "            )\n",
    "            \n",
    "            mse_test, mse_per_sample, mse_per_view = evaluate_on_test_with_per_view(\n",
    "                vae, vm, gp, train_queue, test_queue,\n",
    "                Dt, Wt, Dtest, Wtest, device\n",
    "            )\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                K = vm.get_kernel_matrix().cpu().numpy()\n",
    "                vs = gp.get_vs().cpu().numpy()\n",
    "                variance_ratio = vs[0] / (vs[0] + vs[1])\n",
    "            \n",
    "            kernel_results['mse_per_seed'].append(mse_test)\n",
    "            kernel_results['mse_per_sample_all'].append(mse_per_sample)\n",
    "            kernel_results['mse_per_view_all'].append(mse_per_view)\n",
    "            kernel_results['kernel_matrices'].append(K)\n",
    "            kernel_results['variance_ratios'].append(variance_ratio)\n",
    "            \n",
    "            print(f\"MSE = {mse_test:.6f}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"âŒ Error: {e}\")\n",
    "            continue\n",
    "    \n",
    "    if kernel_results['mse_per_seed']:\n",
    "        mse_array = np.array(kernel_results['mse_per_seed'])\n",
    "        kernel_results['mean_mse'] = np.mean(mse_array)\n",
    "        kernel_results['std_mse'] = np.std(mse_array)\n",
    "        kernel_results['var_mse'] = np.var(mse_array)\n",
    "        \n",
    "        # Aggregate per-view MSE across seeds\n",
    "        all_views = set()\n",
    "        for pv in kernel_results['mse_per_view_all']:\n",
    "            all_views.update(pv.keys())\n",
    "        \n",
    "        kernel_results['mean_mse_per_view'] = {}\n",
    "        kernel_results['std_mse_per_view'] = {}\n",
    "        for view in all_views:\n",
    "            view_mses = []\n",
    "            for pv in kernel_results['mse_per_view_all']:\n",
    "                if view in pv:\n",
    "                    view_mses.extend(pv[view])\n",
    "            kernel_results['mean_mse_per_view'][view] = np.mean(view_mses)\n",
    "            kernel_results['std_mse_per_view'][view] = np.std(view_mses)\n",
    "        \n",
    "        print(f\"\\n  ğŸ“Š {kernel_config['display_name']} Summary:\")\n",
    "        print(f\"     Mean MSE: {kernel_results['mean_mse']:.6f} Â± {kernel_results['std_mse']:.6f}\")\n",
    "        print(f\"     Seeds: {len(kernel_results['mse_per_seed'])}\")\n",
    "    \n",
    "    results[kernel_name] = kernel_results\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"âœ… Evaluation complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91fce4a6",
   "metadata": {},
   "source": [
    "## 6. Summary Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd70c04b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create summary DataFrame\n",
    "summary_data = []\n",
    "\n",
    "for kernel_name, kernel_config in KERNEL_CONFIGS.items():\n",
    "    if kernel_name in results and results[kernel_name]['mse_per_seed']:\n",
    "        r = results[kernel_name]\n",
    "        summary_data.append({\n",
    "            'Kernel': kernel_config['display_name'],\n",
    "            'Mean MSE': r['mean_mse'],\n",
    "            'Std MSE': r['std_mse'],\n",
    "            'Var MSE': r['var_mse'],\n",
    "            'Min MSE': np.min(r['mse_per_seed']),\n",
    "            'Max MSE': np.max(r['mse_per_seed']),\n",
    "            'N Seeds': len(r['mse_per_seed']),\n",
    "            'Mean Var Ratio': np.mean(r['variance_ratios']),\n",
    "        })\n",
    "\n",
    "summary_df = pd.DataFrame(summary_data)\n",
    "summary_df = summary_df.sort_values('Mean MSE')\n",
    "\n",
    "print(\"\\nğŸ“Š Task 2 (Interpolation) - Kernel Comparison Summary\")\n",
    "print(\"=\"*80)\n",
    "print(summary_df.to_string(index=False))\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb0a590b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display formatted table\n",
    "styled_df = summary_df.style.format({\n",
    "    'Mean MSE': '{:.6f}',\n",
    "    'Std MSE': '{:.6f}',\n",
    "    'Var MSE': '{:.8f}',\n",
    "    'Min MSE': '{:.6f}',\n",
    "    'Max MSE': '{:.6f}',\n",
    "    'Mean Var Ratio': '{:.3f}',\n",
    "}).background_gradient(subset=['Mean MSE'], cmap='RdYlGn_r')\n",
    "\n",
    "styled_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "285e8a40",
   "metadata": {},
   "source": [
    "## 7. Visualization: MSE Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baa41fa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bar plot with error bars - clean style\n",
    "fig, ax = plt.subplots(1, 1, figsize=(6, 5))\n",
    "\n",
    "kernel_names = []\n",
    "means = []\n",
    "stds = []\n",
    "\n",
    "for kernel_name, kernel_config in KERNEL_CONFIGS.items():\n",
    "    if kernel_name in results and results[kernel_name]['mse_per_seed']:\n",
    "        kernel_names.append(kernel_config['display_name'])\n",
    "        means.append(results[kernel_name]['mean_mse'])\n",
    "        stds.append(results[kernel_name]['std_mse'])\n",
    "\n",
    "x = np.arange(len(kernel_names))\n",
    "bars = ax.bar(x, means, yerr=stds, capsize=5, color='#808080', alpha=0.8, edgecolor='black', linewidth=0.5)\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(kernel_names, rotation=0, ha='center')\n",
    "ax.set_ylabel('MSE [test set]')\n",
    "ax.set_ylim(bottom=0)\n",
    "ax.spines['top'].set_visible(False)\n",
    "ax.spines['right'].set_visible(False)\n",
    "ax.grid(False)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('./notebooks/analysis/task2_mse_bar.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"ğŸ“Š Saved: task2_mse_bar.png (300 DPI)\")\n",
    "\n",
    "# Box plot - clean style\n",
    "fig, ax = plt.subplots(1, 1, figsize=(6, 5))\n",
    "\n",
    "data_for_box = []\n",
    "labels_for_box = []\n",
    "\n",
    "for kernel_name, kernel_config in KERNEL_CONFIGS.items():\n",
    "    if kernel_name in results and results[kernel_name]['mse_per_seed']:\n",
    "        data_for_box.append(results[kernel_name]['mse_per_seed'])\n",
    "        labels_for_box.append(kernel_config['display_name'])\n",
    "\n",
    "bp = ax.boxplot(data_for_box, labels=labels_for_box, patch_artist=True)\n",
    "for patch in bp['boxes']:\n",
    "    patch.set_facecolor('#808080')\n",
    "    patch.set_alpha(0.7)\n",
    "    patch.set_linewidth(0.5)\n",
    "\n",
    "for element in ['whiskers', 'fliers', 'means', 'medians', 'caps']:\n",
    "    plt.setp(bp[element], linewidth=0.5)\n",
    "\n",
    "ax.set_ylabel('MSE [test set]')\n",
    "ax.spines['top'].set_visible(False)\n",
    "ax.spines['right'].set_visible(False)\n",
    "ax.grid(False)\n",
    "ax.tick_params(axis='x', rotation=0)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('./notebooks/analysis/task2_mse_boxplot.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"ğŸ“Š Saved: task2_mse_boxplot.png (300 DPI)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65c93b89",
   "metadata": {},
   "source": [
    "## 8. Per-View MSE Analysis (Interpolation Quality)\n",
    "\n",
    "This is the key analysis for Task 2: How well does each kernel interpolate to intermediate views?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17fe7ee7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Per-view MSE comparison plots\n",
    "test_views_sorted = sorted([int(v) for v in test_views])\n",
    "\n",
    "# Define consistent colors for each kernel\n",
    "line_colors = {\n",
    "    'Full Rank': '#1f77b4',      # Blue\n",
    "    'Periodic': '#ff7f0e',        # Orange\n",
    "    'SM (Wrapped)': '#9467bd',    # Purple\n",
    "    'SM (Free)': '#8c564b',       # Brown\n",
    "}\n",
    "\n",
    "# Bar plot: Per-view MSE\n",
    "fig, ax = plt.subplots(1, 1, figsize=(12, 5))\n",
    "\n",
    "n_kernels = len([k for k in KERNEL_CONFIGS if k in results and 'mean_mse_per_view' in results[k]])\n",
    "bar_width = 0.8 / n_kernels\n",
    "x_positions = np.arange(len(test_views_sorted))\n",
    "\n",
    "for i, (kernel_name, kernel_config) in enumerate(KERNEL_CONFIGS.items()):\n",
    "    if kernel_name in results and 'mean_mse_per_view' in results[kernel_name]:\n",
    "        mean_per_view = results[kernel_name]['mean_mse_per_view']\n",
    "        mses = [mean_per_view.get(v, 0) for v in test_views_sorted]\n",
    "        display_name = kernel_config['display_name']\n",
    "        color = line_colors.get(display_name, kernel_config['color'])\n",
    "        ax.bar(x_positions + i * bar_width, mses, bar_width, \n",
    "               label=display_name, color=color, alpha=0.8)\n",
    "\n",
    "ax.set_xlabel('Test View Angle')\n",
    "ax.set_ylabel('Mean MSE')\n",
    "ax.set_xticks(x_positions + bar_width * 1.5)\n",
    "ax.set_xticklabels([f\"{int(v)*20}Â°\" for v in test_views_sorted], rotation=45)\n",
    "ax.legend()\n",
    "ax.spines['top'].set_visible(False)\n",
    "ax.spines['right'].set_visible(False)\n",
    "ax.grid(False)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('./notebooks/analysis/task2_per_view_bar.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"ğŸ“Š Saved: task2_per_view_bar.png (300 DPI)\")\n",
    "\n",
    "# Line plot showing interpolation pattern\n",
    "fig, ax = plt.subplots(1, 1, figsize=(10, 5))\n",
    "\n",
    "for kernel_name, kernel_config in KERNEL_CONFIGS.items():\n",
    "    if kernel_name in results and 'mean_mse_per_view' in results[kernel_name]:\n",
    "        mean_per_view = results[kernel_name]['mean_mse_per_view']\n",
    "        angles = [v * 20 for v in sorted(mean_per_view.keys())]\n",
    "        mses = [mean_per_view[v] for v in sorted(mean_per_view.keys())]\n",
    "        display_name = kernel_config['display_name']\n",
    "        color = line_colors.get(display_name, kernel_config['color'])\n",
    "        ax.plot(angles, mses, 'o-', label=display_name, \n",
    "                color=color, markersize=8, linewidth=2)\n",
    "\n",
    "# Mark training views\n",
    "train_angles = [v * 20 for v in train_view_set]\n",
    "for angle in train_angles:\n",
    "    ax.axvline(x=angle, color='gray', linestyle='--', alpha=0.3)\n",
    "\n",
    "ax.set_xlabel('View Angle (Â°)')\n",
    "ax.set_ylabel('Mean MSE')\n",
    "ax.legend(loc='best')\n",
    "ax.set_xlim(-10, 350)\n",
    "ax.spines['top'].set_visible(False)\n",
    "ax.spines['right'].set_visible(False)\n",
    "ax.grid(False)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('./notebooks/analysis/task2_per_view_line.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"ğŸ“Š Saved: task2_per_view_line.png (300 DPI)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5c7dadb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Per-view MSE table\n",
    "per_view_data = []\n",
    "test_views_sorted = sorted([int(v) for v in test_views])\n",
    "\n",
    "for view in test_views_sorted:\n",
    "    row = {'View': f\"{view*20}Â°\", 'View Index': view}\n",
    "    for kernel_name, kernel_config in KERNEL_CONFIGS.items():\n",
    "        if kernel_name in results and 'mean_mse_per_view' in results[kernel_name]:\n",
    "            row[kernel_config['display_name']] = results[kernel_name]['mean_mse_per_view'].get(view, np.nan)\n",
    "    per_view_data.append(row)\n",
    "\n",
    "per_view_df = pd.DataFrame(per_view_data)\n",
    "print(\"\\nğŸ“Š Per-View MSE Breakdown (Interpolation Targets):\")\n",
    "print(\"=\"*80)\n",
    "print(per_view_df.to_string(index=False))\n",
    "\n",
    "# Find best kernel per view\n",
    "print(\"\\nğŸ† Best Kernel per View:\")\n",
    "kernel_cols = [c for c in per_view_df.columns if c not in ['View', 'View Index']]\n",
    "for _, row in per_view_df.iterrows():\n",
    "    best_kernel = min(kernel_cols, key=lambda k: row[k] if pd.notna(row[k]) else float('inf'))\n",
    "    print(f\"   {row['View']}: {best_kernel} ({row[best_kernel]:.6f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3988853a",
   "metadata": {},
   "source": [
    "## 9. Interpolation Smoothness Analysis\n",
    "\n",
    "Analyze how smoothly each kernel interpolates between training views."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "019c0cb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze interpolation as a function of distance to nearest training view\n",
    "# For interpolation task, all test views are 20Â° away from nearest train views\n",
    "\n",
    "train_view_set = set(train_data.Rid.numpy().astype(int))\n",
    "\n",
    "print(\"ğŸ“Š Interpolation Distance Analysis:\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "for kernel_name, kernel_config in KERNEL_CONFIGS.items():\n",
    "    if kernel_name in results and 'mean_mse_per_view' in results[kernel_name]:\n",
    "        mean_per_view = results[kernel_name]['mean_mse_per_view']\n",
    "        \n",
    "        # Compute average MSE\n",
    "        avg_mse = np.mean(list(mean_per_view.values()))\n",
    "        std_mse = np.std(list(mean_per_view.values()))\n",
    "        \n",
    "        # Variance across views (uniformity of interpolation)\n",
    "        view_variance = np.var(list(mean_per_view.values()))\n",
    "        \n",
    "        print(f\"\\n{kernel_config['display_name']}:\")\n",
    "        print(f\"   Mean MSE across views: {avg_mse:.6f} Â± {std_mse:.6f}\")\n",
    "        print(f\"   Variance across views: {view_variance:.8f} (lower = more uniform interpolation)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58d37abe",
   "metadata": {},
   "source": [
    "## 10. Kernel Matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8ef15b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot kernel matrices (matching callback_gppvae style)\n",
    "import pylab as pl\n",
    "pl.rcdefaults()\n",
    "\n",
    "n_kernels = len([k for k in results if results[k]['kernel_matrices']])\n",
    "if n_kernels > 0:\n",
    "    fig, axes = pl.subplots(2, 2, figsize=(10, 10))\n",
    "    axes = axes.flatten()\n",
    "\n",
    "    ax_idx = 0\n",
    "    for kernel_name, kernel_config in KERNEL_CONFIGS.items():\n",
    "        if kernel_name in results and results[kernel_name]['kernel_matrices']:\n",
    "            K = results[kernel_name]['kernel_matrices'][0]\n",
    "\n",
    "            ax = axes[ax_idx]\n",
    "            im = ax.imshow(K, vmin=-0.4, vmax=1, aspect='auto')\n",
    "\n",
    "            tick_positions = [0, 4, 8, 12, 17]\n",
    "            tick_labels = [f\"{p*20}Â°\" for p in tick_positions]\n",
    "            ax.set_xticks(tick_positions)\n",
    "            ax.set_xticklabels(tick_labels, fontsize=8)\n",
    "            ax.set_yticks(tick_positions)\n",
    "            ax.set_yticklabels(tick_labels, fontsize=8)\n",
    "            ax.tick_params(labelsize=8)\n",
    "            ax.set_title(kernel_config['display_name'], fontsize=10)\n",
    "\n",
    "            pl.colorbar(im, ax=ax, fraction=0.046, pad=0.04)\n",
    "            ax_idx += 1\n",
    "\n",
    "    # Hide unused subplots\n",
    "    for i in range(ax_idx, 4):\n",
    "        axes[i].axis('off')\n",
    "\n",
    "    pl.tight_layout()\n",
    "    pl.savefig('./notebooks/analysis/task2_kernel_matrices.png', dpi=300, bbox_inches='tight')\n",
    "    pl.show()\n",
    "    print(\"ğŸ“Š Saved: task2_kernel_matrices.png (300 DPI)\")\n",
    "else:\n",
    "    print(\"âš ï¸ No kernel matrices to display\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad54e98a",
   "metadata": {},
   "source": [
    "## 11. Sample Reconstructions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b642bcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get reconstructions from best seed of each kernel\n",
    "n_samples = 8\n",
    "n_kernels = len([k for k in results if results[k]['mse_per_seed']])\n",
    "\n",
    "if n_kernels > 0:\n",
    "    fig, axes = plt.subplots(n_kernels + 1, n_samples, figsize=(n_samples * 1.5, (n_kernels + 1) * 1.5))\n",
    "    \n",
    "    # Ground truth\n",
    "    n_total = len(test_data)\n",
    "    sample_stride = max(1, n_total // n_samples)\n",
    "    sample_indices = list(range(0, n_total, sample_stride))[:n_samples]\n",
    "    Y_gt = test_data.Y[sample_indices].numpy().transpose(0, 2, 3, 1)\n",
    "    \n",
    "    for i in range(n_samples):\n",
    "        axes[0, i].imshow(np.clip(Y_gt[i], 0, 1))\n",
    "        axes[0, i].axis('off')\n",
    "        # Show view angle\n",
    "        view_idx = int(test_data.Rid[sample_indices[i]].item())\n",
    "        axes[0, i].set_title(f\"{view_idx*20}Â°\", fontsize=8)\n",
    "    axes[0, 0].set_ylabel('GT', fontsize=10)\n",
    "    \n",
    "    row_idx = 1\n",
    "    for kernel_name, kernel_config in KERNEL_CONFIGS.items():\n",
    "        if kernel_name in results and results[kernel_name]['mse_per_seed']:\n",
    "            best_seed_idx = np.argmin(results[kernel_name]['mse_per_seed'])\n",
    "            best_run = find_run_folders(CONFIG['results_base'], kernel_config['folder'])[best_seed_idx]\n",
    "            \n",
    "            vae, vm, gp = load_models(best_run, kernel_config, P, Q, CONFIG['xdim'], device)\n",
    "            Y_orig, Y_recon = get_reconstructions(\n",
    "                vae, vm, gp, train_queue, test_queue,\n",
    "                Dt, Wt, Dtest, Wtest, device, n_samples=n_samples\n",
    "            )\n",
    "            \n",
    "            for i in range(n_samples):\n",
    "                axes[row_idx, i].imshow(np.clip(Y_recon[i], 0, 1))\n",
    "                axes[row_idx, i].axis('off')\n",
    "            \n",
    "            mse = results[kernel_name]['mse_per_seed'][best_seed_idx]\n",
    "            axes[row_idx, 0].set_ylabel(f\"{kernel_config['display_name']}\\n({mse:.4f})\", fontsize=8)\n",
    "            row_idx += 1\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('./notebooks/analysis/task2_reconstructions.png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    print(\"ğŸ“Š Saved: task2_reconstructions.png (300 DPI)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2025b576",
   "metadata": {},
   "source": [
    "## 12. Statistical Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba4a33c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import stats\n",
    "\n",
    "kernel_names_list = [k for k in KERNEL_CONFIGS if k in results and results[k]['mse_per_seed']]\n",
    "\n",
    "if len(kernel_names_list) >= 2:\n",
    "    print(\"\\nğŸ“Š Pairwise T-Tests (p-values)\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    for i, k1 in enumerate(kernel_names_list):\n",
    "        for j, k2 in enumerate(kernel_names_list):\n",
    "            if i < j:\n",
    "                mse1 = results[k1]['mse_per_seed']\n",
    "                mse2 = results[k2]['mse_per_seed']\n",
    "                \n",
    "                t_stat, p_value = stats.ttest_ind(mse1, mse2)\n",
    "                \n",
    "                k1_name = KERNEL_CONFIGS[k1]['display_name']\n",
    "                k2_name = KERNEL_CONFIGS[k2]['display_name']\n",
    "                sig = \"*\" if p_value < 0.05 else \"\"\n",
    "                print(f\"{k1_name} vs {k2_name}: p = {p_value:.4f} {sig}\")\n",
    "    \n",
    "    print(\"\\n* indicates p < 0.05 (statistically significant)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61f0453f",
   "metadata": {},
   "source": [
    "## 13. Callback-Style Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "684acbec",
   "metadata": {},
   "outputs": [],
   "source": [
    "from callbacks import callback_gppvae, _compose_multi\n",
    "import pylab as pl\n",
    "\n",
    "def generate_callback_plot_for_kernel(kernel_name, kernel_config, results, \n",
    "                                       train_queue, test_queue,\n",
    "                                       Dt, Wt, Dtest, Wtest, P, Q, xdim, device,\n",
    "                                       output_file):\n",
    "    if kernel_name not in results or not results[kernel_name]['mse_per_seed']:\n",
    "        return None\n",
    "    \n",
    "    pl.rcdefaults()\n",
    "    \n",
    "    best_seed_idx = np.argmin(results[kernel_name]['mse_per_seed'])\n",
    "    best_run = find_run_folders(CONFIG['results_base'], kernel_config['folder'])[best_seed_idx]\n",
    "    \n",
    "    vae, vm, gp = load_models(best_run, kernel_config, P, Q, xdim, device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        X = vm.x().cpu().numpy()\n",
    "        W = vm.v().cpu().numpy()\n",
    "        XX = X @ X.T\n",
    "        WW = W @ W.T\n",
    "        covs = {\"XX\": XX[:20, :20], \"WW\": WW}\n",
    "        \n",
    "        Zm = encode_dataset(vae, train_queue, device)\n",
    "        Vt = vm(Dt, Wt).detach()\n",
    "        Vtest = vm(Dtest, Wtest).detach()\n",
    "        \n",
    "        vs = gp.get_vs()\n",
    "        U, UBi, _ = gp.U_UBi_Shb([Vt], vs)\n",
    "        Kiz = gp.solve(Zm, U, UBi, vs)\n",
    "        Zo_test = vs[0] * Vtest.mm(Vt.transpose(0, 1).mm(Kiz))\n",
    "        \n",
    "        n_total = len(test_queue.dataset)\n",
    "        sample_stride = max(1, n_total // 24)\n",
    "        sample_indices = np.arange(0, n_total, sample_stride)[:24]\n",
    "        sample_indices_tensor = torch.tensor(sample_indices, dtype=torch.long).to(device)\n",
    "        \n",
    "        Yv = test_queue.dataset.Y[sample_indices].numpy().transpose((0, 2, 3, 1))\n",
    "        Y_input = test_queue.dataset.Y[sample_indices].to(device)\n",
    "        Zm_test, _ = vae.encode(Y_input)\n",
    "        Yr = vae.decode(Zm_test).data.cpu().numpy().transpose((0, 2, 3, 1))\n",
    "        Yo = vae.decode(Zo_test[sample_indices_tensor]).data.cpu().numpy().transpose((0, 2, 3, 1))\n",
    "        \n",
    "        imgs = {\"Yv\": Yv, \"Yr\": Yr, \"Yo\": Yo}\n",
    "        \n",
    "        n_seeds = len(results[kernel_name]['mse_per_seed'])\n",
    "        history = {\n",
    "            \"loss\": results[kernel_name]['mse_per_seed'],\n",
    "            \"vs\": [[vs.cpu().numpy()[0], vs.cpu().numpy()[1]]] * n_seeds,\n",
    "            \"recon_term\": [0.0] * n_seeds,\n",
    "            \"gp_nll\": [0.0] * n_seeds,\n",
    "            \"mse_out\": results[kernel_name]['mse_per_seed'],\n",
    "            \"mse\": [results[kernel_name]['mean_mse']] * n_seeds,\n",
    "            \"mse_val\": results[kernel_name]['mse_per_seed'],\n",
    "        }\n",
    "    \n",
    "    callback_gppvae(epoch=n_seeds - 1, history=history, covs=covs, imgs=imgs, ffile=output_file)\n",
    "    return output_file\n",
    "\n",
    "print(\"âœ… Callback plot function defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b709183e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate callback plots\n",
    "for kernel_name, kernel_config in KERNEL_CONFIGS.items():\n",
    "    if kernel_name in results and results[kernel_name]['mse_per_seed']:\n",
    "        print(f\"\\nğŸ“Š Generating callback plot for {kernel_config['display_name']}...\")\n",
    "        filename = f'./notebooks/analysis/task2_callback_{kernel_name}.png'\n",
    "        result = generate_callback_plot_for_kernel(\n",
    "            kernel_name, kernel_config, results,\n",
    "            train_queue, test_queue,\n",
    "            Dt, Wt, Dtest, Wtest, P, Q, CONFIG['xdim'], device,\n",
    "            output_file=filename\n",
    "        )\n",
    "        if result:\n",
    "            print(f\"   âœ… Saved: {filename}\")\n",
    "            from IPython.display import Image, display\n",
    "            display(Image(filename=filename))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "693395a5",
   "metadata": {},
   "source": [
    "## 14. Export Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27acdee5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save detailed results\n",
    "detailed_results = []\n",
    "\n",
    "for kernel_name, kernel_config in KERNEL_CONFIGS.items():\n",
    "    if kernel_name in results and results[kernel_name]['mse_per_seed']:\n",
    "        for seed_idx, mse in enumerate(results[kernel_name]['mse_per_seed']):\n",
    "            detailed_results.append({\n",
    "                'kernel': kernel_config['display_name'],\n",
    "                'seed': seed_idx,\n",
    "                'test_mse': mse,\n",
    "                'variance_ratio': results[kernel_name]['variance_ratios'][seed_idx],\n",
    "            })\n",
    "\n",
    "detailed_df = pd.DataFrame(detailed_results)\n",
    "detailed_df.to_csv('./notebooks/analysis/task2_detailed_results.csv', index=False)\n",
    "print(\"ğŸ“Š Saved: task2_detailed_results.csv\")\n",
    "\n",
    "summary_df.to_csv('./notebooks/analysis/task2_summary.csv', index=False)\n",
    "print(\"ğŸ“Š Saved: task2_summary.csv\")\n",
    "\n",
    "# Save per-view results\n",
    "per_view_df.to_csv('./notebooks/analysis/task2_per_view_mse.csv', index=False)\n",
    "print(\"ğŸ“Š Saved: task2_per_view_mse.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cb765e1",
   "metadata": {},
   "source": [
    "## 15. Final Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b74e34b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"ğŸ“Š TASK 2 (INTERPOLATION) - FINAL SUMMARY\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(\"\\nğŸ¯ Research Question:\")\n",
    "print(\"   Does imposing smoothness via structured kernels improve performance\")\n",
    "print(\"   when test views lie WITHIN the training range?\")\n",
    "\n",
    "print(\"\\nğŸ† Kernel Ranking (by Mean Test MSE):\")\n",
    "print(\"-\"*50)\n",
    "\n",
    "ranked = sorted(\n",
    "    [(k, results[k]['mean_mse'], results[k]['std_mse']) \n",
    "     for k in results if results[k]['mse_per_seed']],\n",
    "    key=lambda x: x[1]\n",
    ")\n",
    "\n",
    "for rank, (kernel_name, mean_mse, std_mse) in enumerate(ranked, 1):\n",
    "    display_name = KERNEL_CONFIGS[kernel_name]['display_name']\n",
    "    medal = \"ğŸ¥‡\" if rank == 1 else \"ğŸ¥ˆ\" if rank == 2 else \"ğŸ¥‰\" if rank == 3 else \"  \"\n",
    "    print(f\"{medal} {rank}. {display_name:15s}: {mean_mse:.6f} Â± {std_mse:.6f}\")\n",
    "\n",
    "# Analysis of interpolation uniformity\n",
    "print(\"\\nğŸ“Š Interpolation Uniformity (variance across test views):\")\n",
    "print(\"-\"*50)\n",
    "for kernel_name, kernel_config in KERNEL_CONFIGS.items():\n",
    "    if kernel_name in results and 'mean_mse_per_view' in results[kernel_name]:\n",
    "        var = np.var(list(results[kernel_name]['mean_mse_per_view'].values()))\n",
    "        print(f\"   {kernel_config['display_name']:15s}: {var:.8f}\")\n",
    "\n",
    "print(\"\\nğŸ“ Generated Files:\")\n",
    "print(\"-\"*50)\n",
    "print(\"   - task2_mse_comparison.png\")\n",
    "print(\"   - task2_per_view_mse.png\")\n",
    "print(\"   - task2_kernel_matrices.png\")\n",
    "print(\"   - task2_reconstructions.png\")\n",
    "print(\"   - task2_detailed_results.csv\")\n",
    "print(\"   - task2_summary.csv\")\n",
    "print(\"   - task2_per_view_mse.csv\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"âœ… Analysis Complete!\")\n",
    "print(\"=\"*70)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
