{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "20c62e51",
   "metadata": {},
   "source": [
    "# Generate Best Weights for Task 2 & Task 3 Runs\n",
    "\n",
    "This notebook scans the results folders for Task 2 (interpolation) and Task 3 (extrapolation), finds runs that are **missing `*.best.pt` files**, and evaluates all available checkpoints to determine and save the best weights.\n",
    "\n",
    "## Kernels Processed:\n",
    "- **SM Free** (sm_free)\n",
    "- **SM Wrapped** (sm_wrapped)  \n",
    "- **Periodic** (periodic)\n",
    "\n",
    "## Process:\n",
    "1. Scan each run folder for `gp_weights.best.pt`\n",
    "2. If missing, load all available checkpoints\n",
    "3. Evaluate each checkpoint on validation set\n",
    "4. Save the best checkpoint as `*.best.pt`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36fe91ab",
   "metadata": {},
   "source": [
    "## 1. Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cb763c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import glob\n",
    "import shutil\n",
    "import re\n",
    "\n",
    "import torch\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "390eb583",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Auto-detect project path\n",
    "current_dir = os.getcwd()\n",
    "print(f\"üìç Current directory: {current_dir}\")\n",
    "\n",
    "if current_dir == '/content':\n",
    "    print(\"\\nüîÑ Mounting Google Drive...\")\n",
    "    try:\n",
    "        from google.colab import drive\n",
    "        drive.mount('/content/drive')\n",
    "        drive_path = '/content/drive/MyDrive/gppvae'\n",
    "        if os.path.exists(drive_path):\n",
    "            PROJECT_PATH = drive_path\n",
    "            print(f\"‚úÖ Found project in Google Drive: {PROJECT_PATH}\")\n",
    "        else:\n",
    "            print(f\"‚ö†Ô∏è Project not found at: {drive_path}\")\n",
    "            PROJECT_PATH = '/content'\n",
    "    except Exception as e:\n",
    "        print(f\"Could not mount Drive: {e}\")\n",
    "        PROJECT_PATH = '/content'\n",
    "else:\n",
    "    if 'notebooks' in current_dir:\n",
    "        PROJECT_PATH = os.path.dirname(os.path.dirname(current_dir))\n",
    "    else:\n",
    "        PROJECT_PATH = current_dir\n",
    "    print(f\"üíª Using project path: {PROJECT_PATH}\")\n",
    "\n",
    "# Add code paths\n",
    "coil100_path = os.path.join(PROJECT_PATH, 'GPPVAE/pysrc/coil100')\n",
    "sys.path.insert(0, coil100_path)\n",
    "\n",
    "# Change to project root\n",
    "os.chdir(PROJECT_PATH)\n",
    "print(f\"Working directory: {os.getcwd()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fec7aed7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import models\n",
    "import numpy as np\n",
    "import pickle\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from vae import FaceVAE\n",
    "from vmod import Vmodel\n",
    "from gp import GP\n",
    "from data_parser import COIL100Dataset, get_n_views, get_num_objects\n",
    "\n",
    "print(\"‚úÖ All modules imported\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e62510d",
   "metadata": {},
   "source": [
    "## 2. Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ab549e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration - Auto-detect results folder location\n",
    "# Check both ./results and ./GPPVAE/results\n",
    "if os.path.exists('./GPPVAE/results'):\n",
    "    RESULTS_BASE = './GPPVAE/results'\n",
    "    print(f\"‚úÖ Found results at: {os.path.abspath(RESULTS_BASE)}\")\n",
    "elif os.path.exists('./results'):\n",
    "    RESULTS_BASE = './results'\n",
    "    print(f\"‚úÖ Found results at: {os.path.abspath(RESULTS_BASE)}\")\n",
    "else:\n",
    "    RESULTS_BASE = './results'\n",
    "    print(f\"‚ö†Ô∏è Results folder not found, using default: {RESULTS_BASE}\")\n",
    "\n",
    "BATCH_SIZE = 64\n",
    "XDIM = 64\n",
    "\n",
    "# Tasks and kernels to process\n",
    "TASKS_TO_PROCESS = [\n",
    "    {\n",
    "        'task_name': 'task2',\n",
    "        'data_file': './data/coil-100/coil100_task2_interpolation.h5',\n",
    "        'kernels': ['periodic', 'sm_free', 'sm_wrapped'],\n",
    "    },\n",
    "    {\n",
    "        'task_name': 'task3',\n",
    "        'data_file': './data/coil-100/coil100_task3_extrapolation.h5',\n",
    "        'kernels': ['periodic', 'sm_free', 'sm_wrapped'],\n",
    "    },\n",
    "]\n",
    "\n",
    "# Kernel configurations\n",
    "KERNEL_CONFIGS = {\n",
    "    'periodic': {\n",
    "        'view_kernel': 'periodic',\n",
    "        'kernel_kwargs': {'period': 360.0, 'lengthscale': 1.0, 'variance': 1.0},\n",
    "    },\n",
    "    'sm_free': {\n",
    "        'view_kernel': 'sm_circle',\n",
    "        'kernel_kwargs': {'num_mixtures': 2, 'use_angle_input': True},\n",
    "    },\n",
    "    'sm_wrapped': {\n",
    "        'view_kernel': 'sm_circle',\n",
    "        'kernel_kwargs': {'num_mixtures': 2, 'use_angle_input': True},\n",
    "    },\n",
    "}\n",
    "\n",
    "# Default VAE config\n",
    "DEFAULT_VAE_CFG = {\n",
    "    'img_size': 128,\n",
    "    'nf': 32,\n",
    "    'zdim': 256,\n",
    "    'steps': 5,\n",
    "    'colors': 3,\n",
    "    'act': 'elu',\n",
    "    'vy': 0.001\n",
    "}\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4754707",
   "metadata": {},
   "source": [
    "## 3. Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "436b6917",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_runs_missing_best(results_base, task_name, kernel_name):\n",
    "    \"\"\"\n",
    "    Find all runs that are missing gp_weights.best.pt or vae_weights.best.pt\n",
    "    \"\"\"\n",
    "    folder_name = f\"{task_name}_{kernel_name}\"\n",
    "    kernel_path = os.path.join(results_base, folder_name)\n",
    "    \n",
    "    if not os.path.exists(kernel_path):\n",
    "        print(f\"‚ö†Ô∏è Folder not found: {kernel_path}\")\n",
    "        return []\n",
    "    \n",
    "    runs_missing = []\n",
    "    runs_complete = []\n",
    "    \n",
    "    for run_dir in sorted(os.listdir(kernel_path)):\n",
    "        run_path = os.path.join(kernel_path, run_dir)\n",
    "        if not os.path.isdir(run_path):\n",
    "            continue\n",
    "        \n",
    "        weights_dir = os.path.join(run_path, 'weights')\n",
    "        if not os.path.exists(weights_dir):\n",
    "            continue\n",
    "        \n",
    "        gp_best = os.path.join(weights_dir, 'gp_weights.best.pt')\n",
    "        vae_best = os.path.join(weights_dir, 'vae_weights.best.pt')\n",
    "        \n",
    "        if os.path.exists(gp_best) and os.path.exists(vae_best):\n",
    "            runs_complete.append(run_path)\n",
    "        else:\n",
    "            runs_missing.append(run_path)\n",
    "    \n",
    "    return runs_missing, runs_complete\n",
    "\n",
    "\n",
    "def get_available_checkpoints(weights_dir):\n",
    "    \"\"\"\n",
    "    Get list of available checkpoint epochs from weights directory.\n",
    "    Returns list of tuples: (epoch, gp_path, vae_path)\n",
    "    \"\"\"\n",
    "    gp_files = glob.glob(os.path.join(weights_dir, 'gp_weights.*.pt'))\n",
    "    \n",
    "    checkpoints = []\n",
    "    for gp_path in gp_files:\n",
    "        # Skip best files\n",
    "        if 'best' in gp_path:\n",
    "            continue\n",
    "        \n",
    "        # Extract epoch number\n",
    "        match = re.search(r'gp_weights\\.(\\d+)\\.pt', gp_path)\n",
    "        if match:\n",
    "            epoch = int(match.group(1))\n",
    "            vae_path = gp_path.replace('gp_weights', 'vae_weights')\n",
    "            if os.path.exists(vae_path):\n",
    "                checkpoints.append((epoch, gp_path, vae_path))\n",
    "    \n",
    "    # Sort by epoch\n",
    "    checkpoints.sort(key=lambda x: x[0])\n",
    "    return checkpoints\n",
    "\n",
    "\n",
    "def encode_dataset(vae, data_queue, device, zdim=256):\n",
    "    \"\"\"Encode all images to latent space.\"\"\"\n",
    "    vae.eval()\n",
    "    n = data_queue.dataset.Y.shape[0]\n",
    "    Zm = torch.zeros(n, zdim).to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for data in data_queue:\n",
    "            y = data[0].to(device)\n",
    "            idxs = data[-1].to(device)\n",
    "            zm, _ = vae.encode(y)\n",
    "            Zm[idxs] = zm.detach()\n",
    "    \n",
    "    return Zm\n",
    "\n",
    "\n",
    "def evaluate_checkpoint(vae, vm, gp, train_queue, val_queue,\n",
    "                        Dt, Wt, Dv, Wv, device):\n",
    "    \"\"\"\n",
    "    Evaluate a checkpoint on validation set.\n",
    "    Returns validation MSE (mse_out - the GP prediction MSE).\n",
    "    \"\"\"\n",
    "    vae.eval()\n",
    "    vm.eval()\n",
    "    gp.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        # Encode training data\n",
    "        Zm = encode_dataset(vae, train_queue, device)\n",
    "        \n",
    "        # Compute V matrices\n",
    "        Vt = vm(Dt, Wt).detach()\n",
    "        Vv = vm(Dv, Wv).detach()\n",
    "        \n",
    "        # GP prediction\n",
    "        vs = gp.get_vs()\n",
    "        U, UBi, _ = gp.U_UBi_Shb([Vt], vs)\n",
    "        Kiz = gp.solve(Zm, U, UBi, vs)\n",
    "        Zo_val = vs[0] * Vv.mm(Vt.transpose(0, 1).mm(Kiz))\n",
    "        \n",
    "        # Compute MSE\n",
    "        mse_total = 0.0\n",
    "        n_samples = 0\n",
    "        \n",
    "        for data in val_queue:\n",
    "            idxs = data[-1].to(device)\n",
    "            Yv = data[0].to(device)\n",
    "            Yo = vae.decode(Zo_val[idxs])\n",
    "            mse_batch = ((Yv - Yo) ** 2).view(Yv.shape[0], -1).mean(1)\n",
    "            mse_total += mse_batch.sum().item()\n",
    "            n_samples += Yv.shape[0]\n",
    "        \n",
    "        mse_out = mse_total / n_samples\n",
    "    \n",
    "    return mse_out\n",
    "\n",
    "\n",
    "def process_run(run_path, kernel_config, data_file, device):\n",
    "    \"\"\"\n",
    "    Process a single run: evaluate all checkpoints and save best.\n",
    "    \"\"\"\n",
    "    weights_dir = os.path.join(run_path, 'weights')\n",
    "    checkpoints = get_available_checkpoints(weights_dir)\n",
    "    \n",
    "    if not checkpoints:\n",
    "        print(f\"    ‚ö†Ô∏è No checkpoints found\")\n",
    "        return None\n",
    "    \n",
    "    print(f\"    Found {len(checkpoints)} checkpoints\")\n",
    "    \n",
    "    # Load data\n",
    "    train_data = COIL100Dataset(data_file, split='train', use_angle_encoding=False)\n",
    "    val_data = COIL100Dataset(data_file, split='val', use_angle_encoding=False)\n",
    "    train_queue = DataLoader(train_data, batch_size=BATCH_SIZE, shuffle=False)\n",
    "    val_queue = DataLoader(val_data, batch_size=BATCH_SIZE, shuffle=False)\n",
    "    \n",
    "    P = get_num_objects(data_file)\n",
    "    Q = get_n_views()\n",
    "    \n",
    "    Dt = Variable(train_data.Did.long(), requires_grad=False).to(device)\n",
    "    Wt = Variable(train_data.Rid.long(), requires_grad=False).to(device)\n",
    "    Dv = Variable(val_data.Did.long(), requires_grad=False).to(device)\n",
    "    Wv = Variable(val_data.Rid.long(), requires_grad=False).to(device)\n",
    "    \n",
    "    best_mse = float('inf')\n",
    "    best_epoch = -1\n",
    "    best_gp_path = None\n",
    "    best_vae_path = None\n",
    "    \n",
    "    # Evaluate each checkpoint\n",
    "    for epoch, gp_path, vae_path in checkpoints:\n",
    "        try:\n",
    "            # Load VAE\n",
    "            vae = FaceVAE(**DEFAULT_VAE_CFG).to(device)\n",
    "            vae.load_state_dict(torch.load(vae_path, map_location=device))\n",
    "            \n",
    "            # Load Vmodel and GP\n",
    "            vm = Vmodel(\n",
    "                P=P, Q=Q, p=XDIM,\n",
    "                view_kernel=kernel_config['view_kernel'],\n",
    "                **kernel_config['kernel_kwargs']\n",
    "            ).to(device)\n",
    "            gp = GP(n_rand_effs=1).to(device)\n",
    "            \n",
    "            checkpoint = torch.load(gp_path, map_location=device)\n",
    "            gp.load_state_dict(checkpoint['gp_state'])\n",
    "            vm.load_state_dict(checkpoint['vm_state'])\n",
    "            \n",
    "            # Evaluate\n",
    "            mse = evaluate_checkpoint(\n",
    "                vae, vm, gp, train_queue, val_queue,\n",
    "                Dt, Wt, Dv, Wv, device\n",
    "            )\n",
    "            \n",
    "            if mse < best_mse:\n",
    "                best_mse = mse\n",
    "                best_epoch = epoch\n",
    "                best_gp_path = gp_path\n",
    "                best_vae_path = vae_path\n",
    "            \n",
    "            # Clean up\n",
    "            del vae, vm, gp\n",
    "            torch.cuda.empty_cache()\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"    ‚ö†Ô∏è Error at epoch {epoch}: {e}\")\n",
    "            continue\n",
    "    \n",
    "    if best_epoch >= 0:\n",
    "        print(f\"    ‚úÖ Best epoch: {best_epoch} (MSE: {best_mse:.6f})\")\n",
    "        \n",
    "        # Copy best checkpoint as *.best.pt\n",
    "        best_gp_dest = os.path.join(weights_dir, 'gp_weights.best.pt')\n",
    "        best_vae_dest = os.path.join(weights_dir, 'vae_weights.best.pt')\n",
    "        \n",
    "        shutil.copy2(best_gp_path, best_gp_dest)\n",
    "        shutil.copy2(best_vae_path, best_vae_dest)\n",
    "        \n",
    "        print(f\"    üìÅ Saved: gp_weights.best.pt, vae_weights.best.pt\")\n",
    "        \n",
    "        return {\n",
    "            'run': os.path.basename(run_path),\n",
    "            'best_epoch': best_epoch,\n",
    "            'best_mse': best_mse,\n",
    "        }\n",
    "    \n",
    "    return None\n",
    "\n",
    "print(\"‚úÖ Helper functions defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87bbc7e2",
   "metadata": {},
   "source": [
    "## 4. Scan for Missing Best Weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "608b631e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scan all tasks and kernels\n",
    "all_missing_runs = []\n",
    "\n",
    "print(\"üìä Scanning for runs missing *.best.pt files...\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for task_info in TASKS_TO_PROCESS:\n",
    "    task_name = task_info['task_name']\n",
    "    print(f\"\\nüîç {task_name.upper()}:\")\n",
    "    \n",
    "    for kernel_name in task_info['kernels']:\n",
    "        runs_missing, runs_complete = find_runs_missing_best(\n",
    "            RESULTS_BASE, task_name, kernel_name\n",
    "        )\n",
    "        \n",
    "        print(f\"   {kernel_name}: {len(runs_complete)} complete, {len(runs_missing)} missing\")\n",
    "        \n",
    "        for run_path in runs_missing:\n",
    "            all_missing_runs.append({\n",
    "                'task': task_name,\n",
    "                'kernel': kernel_name,\n",
    "                'run_path': run_path,\n",
    "                'data_file': task_info['data_file'],\n",
    "            })\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"üìã Total runs to process: {len(all_missing_runs)}\")\n",
    "\n",
    "if all_missing_runs:\n",
    "    print(\"\\nRuns to process:\")\n",
    "    for i, run_info in enumerate(all_missing_runs):\n",
    "        print(f\"  {i+1}. {run_info['task']}/{run_info['kernel']}: {os.path.basename(run_info['run_path'])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6a7cab4",
   "metadata": {},
   "source": [
    "## 5. Process Missing Runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8635546",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process all missing runs\n",
    "results = []\n",
    "\n",
    "print(f\"\\nüöÄ Processing {len(all_missing_runs)} runs...\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "for i, run_info in enumerate(all_missing_runs):\n",
    "    task = run_info['task']\n",
    "    kernel = run_info['kernel']\n",
    "    run_path = run_info['run_path']\n",
    "    data_file = run_info['data_file']\n",
    "    \n",
    "    print(f\"\\n[{i+1}/{len(all_missing_runs)}] {task}/{kernel}: {os.path.basename(run_path)}\")\n",
    "    \n",
    "    # Check data file exists\n",
    "    if not os.path.exists(data_file):\n",
    "        print(f\"    ‚ùå Data file not found: {data_file}\")\n",
    "        continue\n",
    "    \n",
    "    kernel_config = KERNEL_CONFIGS[kernel]\n",
    "    \n",
    "    try:\n",
    "        result = process_run(run_path, kernel_config, data_file, device)\n",
    "        if result:\n",
    "            result['task'] = task\n",
    "            result['kernel'] = kernel\n",
    "            results.append(result)\n",
    "    except Exception as e:\n",
    "        print(f\"    ‚ùå Error: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(f\"‚úÖ Processing complete! Generated best weights for {len(results)} runs.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcfd36b1",
   "metadata": {},
   "source": [
    "## 6. Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20c7d07d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "if results:\n",
    "    df = pd.DataFrame(results)\n",
    "    print(\"\\nüìä Generated Best Weights Summary:\")\n",
    "    print(\"=\" * 70)\n",
    "    print(df.to_string(index=False))\n",
    "    \n",
    "    # Group by task and kernel\n",
    "    print(\"\\nüìà Summary by Task/Kernel:\")\n",
    "    summary = df.groupby(['task', 'kernel']).agg({\n",
    "        'best_mse': ['mean', 'std', 'min', 'max', 'count']\n",
    "    }).round(6)\n",
    "    print(summary)\n",
    "else:\n",
    "    print(\"\\n‚úÖ All runs already have best weights!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bc8a292",
   "metadata": {},
   "source": [
    "## 7. Verify Generated Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0675128",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify all runs now have best weights\n",
    "print(\"\\nüîç Verifying all runs now have best weights...\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for task_info in TASKS_TO_PROCESS:\n",
    "    task_name = task_info['task_name']\n",
    "    print(f\"\\n{task_name.upper()}:\")\n",
    "    \n",
    "    for kernel_name in task_info['kernels']:\n",
    "        runs_missing, runs_complete = find_runs_missing_best(\n",
    "            RESULTS_BASE, task_name, kernel_name\n",
    "        )\n",
    "        \n",
    "        status = \"‚úÖ\" if len(runs_missing) == 0 else \"‚ö†Ô∏è\"\n",
    "        print(f\"   {status} {kernel_name}: {len(runs_complete)} complete, {len(runs_missing)} missing\")\n",
    "        \n",
    "        if runs_missing:\n",
    "            for run in runs_missing:\n",
    "                print(f\"      ‚ùå Still missing: {os.path.basename(run)}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"‚úÖ Verification complete!\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
