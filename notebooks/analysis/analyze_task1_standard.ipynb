{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4b3821aa",
   "metadata": {},
   "source": [
    "# Task 1 (Standard) Analysis - Kernel Comparison\n",
    "\n",
    "This notebook analyzes the performance of different GP kernels on **Task 1 (Standard split)**:\n",
    "\n",
    "## Task Description\n",
    "- **Task 1 (Standard)**: Random 70/15/15 split across all views\n",
    "- **Data**: COIL-100 dataset (100 objects √ó 18 views)\n",
    "- **Goal**: Predict held-out images using GP interpolation\n",
    "\n",
    "## Kernels Compared\n",
    "1. **Full Rank**: Free-form learnable covariance (Q√óQ parameters)\n",
    "2. **Periodic**: Standard periodic kernel with learned lengthscale\n",
    "3. **SM Wrapped**: Spectral Mixture with wrapped lag distance\n",
    "4. **SM Free**: Spectral Mixture (unwrapped)\n",
    "\n",
    "## Analysis\n",
    "- Load best checkpoints from each kernel (5 seeds each)\n",
    "- Evaluate on test set\n",
    "- Compute mean and variance of MSE across seeds\n",
    "- Visualize reconstructions and kernel matrices"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3660509",
   "metadata": {},
   "source": [
    "## 1. Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "efc99848",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pandas'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 6\u001b[39m\n\u001b[32m      4\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpickle\u001b[39;00m\n\u001b[32m      5\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnp\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpd\u001b[39;00m\n\u001b[32m      7\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\n\u001b[32m      8\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mautograd\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Variable\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'pandas'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import glob\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "from matplotlib.patches import Patch\n",
    "import seaborn as sns\n",
    "\n",
    "# Set style\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "sns.set_palette('husl')\n",
    "\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c5ae455",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "current_dir = os.getcwd()\n",
    "print(f\"üìç Current directory: {current_dir}\")\n",
    "\n",
    "if current_dir == '/content':\n",
    "    print(\"\\nüîÑ Mounting Google Drive...\")\n",
    "    try:\n",
    "        from google.colab import drive\n",
    "        drive.mount('/content/drive')\n",
    "        drive_path = '/content/drive/MyDrive/gppvae'\n",
    "        if os.path.exists(drive_path):\n",
    "            PROJECT_PATH = drive_path\n",
    "            print(f\"‚úÖ Found project in Google Drive: {PROJECT_PATH}\")\n",
    "        else:\n",
    "            print(f\"‚ö†Ô∏è Project not found at: {drive_path}\")\n",
    "            PROJECT_PATH = '/content'\n",
    "    except Exception as e:\n",
    "        print(f\"Could not mount Drive: {e}\")\n",
    "        PROJECT_PATH = '/content'\n",
    "else:\n",
    "    if 'notebooks' in current_dir:\n",
    "        PROJECT_PATH = os.path.dirname(os.path.dirname(current_dir))\n",
    "    else:\n",
    "        PROJECT_PATH = current_dir\n",
    "    print(f\"üíª Using project path: {PROJECT_PATH}\")\n",
    "\n",
    "# Add code paths\n",
    "coil100_path = os.path.join(PROJECT_PATH, 'GPPVAE/pysrc/coil100')\n",
    "sys.path.insert(0, coil100_path)\n",
    "\n",
    "# Change to project root\n",
    "os.chdir(PROJECT_PATH)\n",
    "print(f\"Working directory: {os.getcwd()}\")\n",
    "print(f\"Code path added: {coil100_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cd29fae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import models\n",
    "from vae import FaceVAE\n",
    "from vmod import Vmodel\n",
    "from gp import GP\n",
    "from data_parser import COIL100Dataset, get_n_views, get_num_objects\n",
    "\n",
    "print(\"‚úÖ All modules imported\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c6ba3e4",
   "metadata": {},
   "source": [
    "## 2. Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36b3f38f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task 1 (Standard) configuration\n",
    "CONFIG = {\n",
    "    'task': 'task1_standard',\n",
    "    'data_path': './data/coil100/coil100_task1_standard.h5',\n",
    "    'batch_size': 64,\n",
    "    'xdim': 64,  # Object embedding dimension\n",
    "}\n",
    "\n",
    "# Auto-detect results folder location\n",
    "# Check both ./results and ./GPPVAE/results\n",
    "if os.path.exists('./GPPVAE/results'):\n",
    "    CONFIG['results_base'] = './GPPVAE/results'\n",
    "    print(f\"‚úÖ Found results at: {os.path.abspath(CONFIG['results_base'])}\")\n",
    "elif os.path.exists('./results'):\n",
    "    CONFIG['results_base'] = './results'\n",
    "    print(f\"‚úÖ Found results at: {os.path.abspath(CONFIG['results_base'])}\")\n",
    "else:\n",
    "    CONFIG['results_base'] = './results'\n",
    "    print(f\"‚ö†Ô∏è Results folder not found, using default: {CONFIG['results_base']}\")\n",
    "\n",
    "# Auto-detect data folder location\n",
    "if os.path.exists('./GPPVAE/data/coil100/coil100_task1_standard.h5'):\n",
    "    CONFIG['data_path'] = './GPPVAE/data/coil100/coil100_task1_standard.h5'\n",
    "    print(f\"‚úÖ Found data at: {os.path.abspath(CONFIG['data_path'])}\")\n",
    "elif os.path.exists('./data/coil100/coil100_task1_standard.h5'):\n",
    "    CONFIG['data_path'] = './data/coil100/coil100_task1_standard.h5'\n",
    "    print(f\"‚úÖ Found data at: {os.path.abspath(CONFIG['data_path'])}\")\n",
    "else:\n",
    "    CONFIG['data_path'] = './data/coil100/coil100_task1_standard.h5'\n",
    "    print(f\"‚ö†Ô∏è Data file not found, using default: {CONFIG['data_path']}\")\n",
    "\n",
    "# Kernel configurations (must match training)\n",
    "KERNEL_CONFIGS = {\n",
    "    'fullrank': {\n",
    "        'folder': 'task1_fullrank',\n",
    "        'view_kernel': 'full_rank',\n",
    "        'kernel_kwargs': {},\n",
    "        'display_name': 'Full Rank',\n",
    "        'color': '#e74c3c',\n",
    "    },\n",
    "    'periodic': {\n",
    "        'folder': 'task1_periodic',\n",
    "        'view_kernel': 'periodic',\n",
    "        'kernel_kwargs': {'period': 360.0, 'lengthscale': 1.0, 'variance': 1.0},\n",
    "        'display_name': 'Periodic',\n",
    "        'color': '#3498db',\n",
    "    },\n",
    "    'sm_wrapped': {\n",
    "        'folder': 'task1_sm_wrapped',\n",
    "        'view_kernel': 'sm_circle',\n",
    "        'kernel_kwargs': {'freq_init': [1/360.0, 1/40.0], 'weight_init':[0.5, 0.5], 'length_init':[90,30]},\n",
    "        'display_name': 'SM (Wrapped)',\n",
    "        'color': '#2ecc71',\n",
    "    },\n",
    "    'sm_free': {\n",
    "        'folder': 'task1_sm_free',\n",
    "        'view_kernel': 'sm_circle',  # Adjust if different\n",
    "        'kernel_kwargs': {'freq_init': [1/360.0, 1/40.0], 'weight_init':[0.5, 0.5], 'use_angle_input': True},\n",
    "        'display_name': 'SM (Free)',\n",
    "        'color': '#9b59b6',\n",
    "    },\n",
    "}\n",
    "\n",
    "# Device\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d9c0e85",
   "metadata": {},
   "source": [
    "## 3. Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53b86891",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load datasets\n",
    "train_data = COIL100Dataset(CONFIG['data_path'], split='train', use_angle_encoding=False)\n",
    "val_data = COIL100Dataset(CONFIG['data_path'], split='val', use_angle_encoding=False)\n",
    "test_data = COIL100Dataset(CONFIG['data_path'], split='test', use_angle_encoding=False)\n",
    "\n",
    "train_queue = DataLoader(train_data, batch_size=CONFIG['batch_size'], shuffle=False)\n",
    "val_queue = DataLoader(val_data, batch_size=CONFIG['batch_size'], shuffle=False)\n",
    "test_queue = DataLoader(test_data, batch_size=CONFIG['batch_size'], shuffle=False)\n",
    "\n",
    "# Get dimensions\n",
    "P = get_num_objects(CONFIG['data_path'])  # Number of objects\n",
    "Q = get_n_views()  # Number of views (18)\n",
    "\n",
    "print(f\"\\nüìä Task 1 (Standard) Dataset:\")\n",
    "print(f\"   Objects (P): {P}\")\n",
    "print(f\"   Views (Q): {Q}\")\n",
    "print(f\"   Train: {len(train_data)} samples\")\n",
    "print(f\"   Val: {len(val_data)} samples\")\n",
    "print(f\"   Test: {len(test_data)} samples\")\n",
    "\n",
    "# Create tensors for indices\n",
    "Dt = Variable(train_data.Did.long(), requires_grad=False).to(device)\n",
    "Wt = Variable(train_data.Rid.long(), requires_grad=False).to(device)\n",
    "Dtest = Variable(test_data.Did.long(), requires_grad=False).to(device)\n",
    "Wtest = Variable(test_data.Rid.long(), requires_grad=False).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ef0f433",
   "metadata": {},
   "source": [
    "## 4. Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f77dcd13",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_run_folders(results_base, kernel_folder):\n",
    "    \"\"\"Find all run folders (seeds) for a kernel.\"\"\"\n",
    "    kernel_path = os.path.join(results_base, kernel_folder)\n",
    "    if not os.path.exists(kernel_path):\n",
    "        print(f\"‚ö†Ô∏è Kernel folder not found: {kernel_path}\")\n",
    "        return []\n",
    "    \n",
    "    runs = sorted([d for d in os.listdir(kernel_path) \n",
    "                   if os.path.isdir(os.path.join(kernel_path, d))])\n",
    "    return [os.path.join(kernel_path, r) for r in runs]\n",
    "\n",
    "\n",
    "def load_vae_config(run_folder=None):\n",
    "    \"\"\"Load VAE config from GPPVAE folder structure or use default.\"\"\"\n",
    "    # Try to find VAE config in GPPVAE folder (as shown in your screenshot)\n",
    "    possible_paths = [\n",
    "        './GPPVAE/pysrc/coil100/vae.cfg.p',  # If there's a shared config\n",
    "        './out/vae/vae.cfg.p',  # Common VAE output folder\n",
    "        './out/vae_colab/*/vae.cfg.p',  # Colab VAE folders\n",
    "    ]\n",
    "    \n",
    "    for path_pattern in possible_paths:\n",
    "        if '*' in path_pattern:\n",
    "            import glob\n",
    "            matches = glob.glob(path_pattern)\n",
    "            if matches:\n",
    "                try:\n",
    "                    import pickle\n",
    "                    with open(matches[0], 'rb') as f:\n",
    "                        vae_cfg = pickle.load(f)\n",
    "                    print(f\"‚úÖ Loaded VAE config from: {matches[0]}\")\n",
    "                    return vae_cfg\n",
    "                except:\n",
    "                    continue\n",
    "        elif os.path.exists(path_pattern):\n",
    "            try:\n",
    "                import pickle\n",
    "                with open(path_pattern, 'rb') as f:\n",
    "                    vae_cfg = pickle.load(f)\n",
    "                print(f\"‚úÖ Loaded VAE config from: {path_pattern}\")\n",
    "                return vae_cfg\n",
    "            except:\n",
    "                continue\n",
    "    \n",
    "    # Default VAE config for COIL-100\n",
    "    print(\"‚ö†Ô∏è Using default VAE config\")\n",
    "    vae_cfg = {\n",
    "        'img_size': 128,\n",
    "        'nf': 32,\n",
    "        'zdim': 256,\n",
    "        'steps': 5,\n",
    "        'colors': 3,\n",
    "        'act': 'elu',\n",
    "        'vy': 0.001\n",
    "    }\n",
    "    return vae_cfg\n",
    "\n",
    "\n",
    "def load_models(run_folder, kernel_config, P, Q, xdim, device):\n",
    "    \"\"\"\n",
    "    Load VAE, Vmodel, and GP from a run folder's best checkpoint.\n",
    "    \"\"\"\n",
    "    weights_dir = os.path.join(run_folder, 'weights')\n",
    "    gp_weights_path = os.path.join(weights_dir, 'gp_weights.best.pt')\n",
    "    vae_weights_path = os.path.join(weights_dir, 'vae_weights.best.pt')\n",
    "    \n",
    "    if not os.path.exists(gp_weights_path):\n",
    "        raise FileNotFoundError(f\"GP weights not found: {gp_weights_path}\")\n",
    "    if not os.path.exists(vae_weights_path):\n",
    "        raise FileNotFoundError(f\"VAE weights not found: {vae_weights_path}\")\n",
    "    \n",
    "    # Load VAE\n",
    "    vae_cfg = load_vae_config(run_folder)\n",
    "    vae = FaceVAE(**vae_cfg).to(device)\n",
    "    vae.load_state_dict(torch.load(vae_weights_path, map_location=device))\n",
    "    vae.eval()\n",
    "    \n",
    "    # Load Vmodel and GP\n",
    "    vm = Vmodel(\n",
    "        P=P, Q=Q, p=xdim,\n",
    "        view_kernel=kernel_config['view_kernel'],\n",
    "        **kernel_config['kernel_kwargs']\n",
    "    ).to(device)\n",
    "    gp = GP(n_rand_effs=1).to(device)\n",
    "    \n",
    "    checkpoint = torch.load(gp_weights_path, map_location=device)\n",
    "    gp.load_state_dict(checkpoint['gp_state'])\n",
    "    vm.load_state_dict(checkpoint['vm_state'])\n",
    "    \n",
    "    vm.eval()\n",
    "    gp.eval()\n",
    "    \n",
    "    return vae, vm, gp\n",
    "\n",
    "\n",
    "def encode_dataset(vae, data_queue, device):\n",
    "    \"\"\"Encode all images in a dataset to latent space.\"\"\"\n",
    "    vae.eval()\n",
    "    n = data_queue.dataset.Y.shape[0]\n",
    "    zdim = 256  # Default zdim\n",
    "    \n",
    "    Zm = torch.zeros(n, zdim).to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for data in data_queue:\n",
    "            y = data[0].to(device)\n",
    "            idxs = data[-1].to(device)\n",
    "            zm, _ = vae.encode(y)\n",
    "            Zm[idxs] = zm.detach()\n",
    "    \n",
    "    return Zm\n",
    "\n",
    "\n",
    "def evaluate_on_test(vae, vm, gp, train_queue, test_queue, \n",
    "                     Dt, Wt, Dtest, Wtest, device):\n",
    "    \"\"\"\n",
    "    Evaluate model on test set.\n",
    "    \n",
    "    Returns:\n",
    "        mse_test: Mean MSE on test set\n",
    "        mse_per_sample: MSE for each test sample\n",
    "        mse_per_view: Dict mapping view index to list of MSEs\n",
    "    \"\"\"\n",
    "    vae.eval()\n",
    "    vm.eval()\n",
    "    gp.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        # Encode training data\n",
    "        Zm = encode_dataset(vae, train_queue, device)\n",
    "        \n",
    "        # Compute V matrices\n",
    "        Vt = vm(Dt, Wt).detach()\n",
    "        Vtest = vm(Dtest, Wtest).detach()\n",
    "        \n",
    "        # GP prediction\n",
    "        vs = gp.get_vs()\n",
    "        U, UBi, _ = gp.U_UBi_Shb([Vt], vs)\n",
    "        Kiz = gp.solve(Zm, U, UBi, vs)\n",
    "        Zo_test = vs[0] * Vtest.mm(Vt.transpose(0, 1).mm(Kiz))\n",
    "        \n",
    "        # Compute MSE\n",
    "        test_Rid = test_queue.dataset.Rid\n",
    "        mse_per_view = {}\n",
    "        mse_per_sample = []\n",
    "        mse_test_total = 0.0\n",
    "        \n",
    "        for data in test_queue:\n",
    "            idxs = data[-1].to(device)\n",
    "            Ytest = data[0].to(device)\n",
    "            Yo = vae.decode(Zo_test[idxs])\n",
    "            mse_batch = ((Ytest - Yo) ** 2).view(Ytest.shape[0], -1).mean(1)\n",
    "            \n",
    "            for i, idx in enumerate(data[-1]):\n",
    "                view = int(test_Rid[idx].item())\n",
    "                mse_val = mse_batch[i].item()\n",
    "                \n",
    "                if view not in mse_per_view:\n",
    "                    mse_per_view[view] = []\n",
    "                mse_per_view[view].append(mse_val)\n",
    "                mse_per_sample.append(mse_val)\n",
    "            \n",
    "            mse_test_total += mse_batch.sum().item()\n",
    "        \n",
    "        mse_test = mse_test_total / len(test_queue.dataset)\n",
    "    \n",
    "    return mse_test, np.array(mse_per_sample), mse_per_view\n",
    "\n",
    "\n",
    "def get_reconstructions(vae, vm, gp, train_queue, test_queue,\n",
    "                        Dt, Wt, Dtest, Wtest, device, n_samples=24):\n",
    "    \"\"\"\n",
    "    Get sample reconstructions for visualization.\n",
    "    \n",
    "    Returns:\n",
    "        Y_orig: Original test images [n_samples, H, W, C]\n",
    "        Y_recon: GP-predicted reconstructions [n_samples, H, W, C]\n",
    "    \"\"\"\n",
    "    vae.eval()\n",
    "    vm.eval()\n",
    "    gp.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        # Encode training data\n",
    "        Zm = encode_dataset(vae, train_queue, device)\n",
    "        \n",
    "        # Compute V matrices\n",
    "        Vt = vm(Dt, Wt).detach()\n",
    "        Vtest = vm(Dtest, Wtest).detach()\n",
    "        \n",
    "        # GP prediction\n",
    "        vs = gp.get_vs()\n",
    "        U, UBi, _ = gp.U_UBi_Shb([Vt], vs)\n",
    "        Kiz = gp.solve(Zm, U, UBi, vs)\n",
    "        Zo_test = vs[0] * Vtest.mm(Vt.transpose(0, 1).mm(Kiz))\n",
    "        \n",
    "        # Get sample images\n",
    "        n_total = len(test_queue.dataset)\n",
    "        sample_stride = max(1, n_total // n_samples)\n",
    "        sample_indices = list(range(0, n_total, sample_stride))[:n_samples]\n",
    "        \n",
    "        Y_orig = test_queue.dataset.Y[sample_indices].numpy().transpose(0, 2, 3, 1)\n",
    "        \n",
    "        # Decode predictions\n",
    "        sample_indices_tensor = torch.tensor(sample_indices, dtype=torch.long).to(device)\n",
    "        Y_recon = vae.decode(Zo_test[sample_indices_tensor])\n",
    "        Y_recon = Y_recon.cpu().numpy().transpose(0, 2, 3, 1)\n",
    "    \n",
    "    return Y_orig, Y_recon\n",
    "\n",
    "print(\"‚úÖ Helper functions defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e1d5cbc",
   "metadata": {},
   "source": [
    "## 5. Evaluate All Kernels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d26604d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Results storage\n",
    "results = {}\n",
    "\n",
    "for kernel_name, kernel_config in KERNEL_CONFIGS.items():\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Evaluating: {kernel_config['display_name']}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    # Find all runs (seeds)\n",
    "    run_folders = find_run_folders(CONFIG['results_base'], kernel_config['folder'])\n",
    "    \n",
    "    if not run_folders:\n",
    "        print(f\"‚ö†Ô∏è No runs found for {kernel_name}\")\n",
    "        continue\n",
    "    \n",
    "    print(f\"Found {len(run_folders)} runs (seeds)\")\n",
    "    \n",
    "    kernel_results = {\n",
    "        'mse_per_seed': [],\n",
    "        'mse_per_sample_all': [],\n",
    "        'kernel_matrices': [],\n",
    "        'variance_ratios': [],\n",
    "    }\n",
    "    \n",
    "    for i, run_folder in enumerate(run_folders):\n",
    "        try:\n",
    "            print(f\"  Seed {i}: {os.path.basename(run_folder)}...\", end=\" \")\n",
    "            \n",
    "            # Load models\n",
    "            vae, vm, gp = load_models(\n",
    "                run_folder, kernel_config, P, Q, \n",
    "                CONFIG['xdim'], device\n",
    "            )\n",
    "            \n",
    "            # Evaluate\n",
    "            mse_test, mse_per_sample, mse_per_view = evaluate_on_test(\n",
    "                vae, vm, gp, train_queue, test_queue,\n",
    "                Dt, Wt, Dtest, Wtest, device\n",
    "            )\n",
    "            \n",
    "            # Get kernel matrix\n",
    "            with torch.no_grad():\n",
    "                K = vm.get_kernel_matrix().cpu().numpy()\n",
    "                vs = gp.get_vs().cpu().numpy()\n",
    "                variance_ratio = vs[0] / (vs[0] + vs[1])\n",
    "            \n",
    "            kernel_results['mse_per_seed'].append(mse_test)\n",
    "            kernel_results['mse_per_sample_all'].append(mse_per_sample)\n",
    "            kernel_results['kernel_matrices'].append(K)\n",
    "            kernel_results['variance_ratios'].append(variance_ratio)\n",
    "            \n",
    "            print(f\"MSE = {mse_test:.6f}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error: {e}\")\n",
    "            continue\n",
    "    \n",
    "    if kernel_results['mse_per_seed']:\n",
    "        mse_array = np.array(kernel_results['mse_per_seed'])\n",
    "        kernel_results['mean_mse'] = np.mean(mse_array)\n",
    "        kernel_results['std_mse'] = np.std(mse_array)\n",
    "        kernel_results['var_mse'] = np.var(mse_array)\n",
    "        \n",
    "        print(f\"\\n  üìä {kernel_config['display_name']} Summary:\")\n",
    "        print(f\"     Mean MSE: {kernel_results['mean_mse']:.6f} ¬± {kernel_results['std_mse']:.6f}\")\n",
    "        print(f\"     Variance: {kernel_results['var_mse']:.8f}\")\n",
    "        print(f\"     Seeds: {len(kernel_results['mse_per_seed'])}\")\n",
    "    \n",
    "    results[kernel_name] = kernel_results\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"‚úÖ Evaluation complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2385a9bb",
   "metadata": {},
   "source": [
    "## 6. Summary Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d7220ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create summary DataFrame\n",
    "summary_data = []\n",
    "\n",
    "for kernel_name, kernel_config in KERNEL_CONFIGS.items():\n",
    "    if kernel_name in results and results[kernel_name]['mse_per_seed']:\n",
    "        r = results[kernel_name]\n",
    "        summary_data.append({\n",
    "            'Kernel': kernel_config['display_name'],\n",
    "            'Mean MSE': r['mean_mse'],\n",
    "            'Std MSE': r['std_mse'],\n",
    "            'Var MSE': r['var_mse'],\n",
    "            'Min MSE': np.min(r['mse_per_seed']),\n",
    "            'Max MSE': np.max(r['mse_per_seed']),\n",
    "            'N Seeds': len(r['mse_per_seed']),\n",
    "            'Mean Var Ratio': np.mean(r['variance_ratios']),\n",
    "        })\n",
    "\n",
    "summary_df = pd.DataFrame(summary_data)\n",
    "summary_df = summary_df.sort_values('Mean MSE')\n",
    "\n",
    "print(\"\\nüìä Task 1 (Standard) - Kernel Comparison Summary\")\n",
    "print(\"=\"*80)\n",
    "print(summary_df.to_string(index=False))\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52919778",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display formatted table with pandas styling\n",
    "styled_df = summary_df.style.format({\n",
    "    'Mean MSE': '{:.6f}',\n",
    "    'Std MSE': '{:.6f}',\n",
    "    'Var MSE': '{:.8f}',\n",
    "    'Min MSE': '{:.6f}',\n",
    "    'Max MSE': '{:.6f}',\n",
    "    'Mean Var Ratio': '{:.3f}',\n",
    "}).background_gradient(subset=['Mean MSE'], cmap='RdYlGn_r')\n",
    "\n",
    "styled_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc836aa6",
   "metadata": {},
   "source": [
    "## 7. Visualization: MSE Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54a0de7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define consistent colors for each kernel\n",
    "line_colors = {\n",
    "    'Full Rank': '#1f77b4',      # Blue\n",
    "    'Periodic': '#ff7f0e',        # Orange\n",
    "    'SM (Wrapped)': '#9467bd',    # Purple\n",
    "    'SM (Free)': '#8c564b',       # Brown\n",
    "}\n",
    "\n",
    "# Bar plot with error bars - clean style\n",
    "fig, ax = plt.subplots(1, 1, figsize=(6, 5))\n",
    "\n",
    "kernel_names = []\n",
    "means = []\n",
    "stds = []\n",
    "colors = []\n",
    "\n",
    "for kernel_name, kernel_config in KERNEL_CONFIGS.items():\n",
    "    if kernel_name in results and results[kernel_name]['mse_per_seed']:\n",
    "        display_name = kernel_config['display_name']\n",
    "        kernel_names.append(display_name)\n",
    "        means.append(results[kernel_name]['mean_mse'])\n",
    "        stds.append(results[kernel_name]['std_mse'])\n",
    "        colors.append(line_colors.get(display_name, kernel_config['color']))\n",
    "\n",
    "x = np.arange(len(kernel_names))\n",
    "bars = ax.bar(x, means, yerr=stds, capsize=5, color=colors, alpha=0.8, edgecolor='black', linewidth=0.5)\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(kernel_names, rotation=0, ha='center')\n",
    "ax.set_ylabel('MSE [test set]')\n",
    "ax.set_ylim(bottom=0)\n",
    "ax.spines['top'].set_visible(False)\n",
    "ax.spines['right'].set_visible(False)\n",
    "ax.grid(False)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('./notebooks/analysis/task1_mse_bar.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"üìä Saved: task1_mse_bar.png (300 DPI)\")\n",
    "\n",
    "# Box plot - clean style\n",
    "fig, ax = plt.subplots(1, 1, figsize=(6, 5))\n",
    "\n",
    "data_for_box = []\n",
    "labels_for_box = []\n",
    "colors_for_box = []\n",
    "\n",
    "for kernel_name, kernel_config in KERNEL_CONFIGS.items():\n",
    "    if kernel_name in results and results[kernel_name]['mse_per_seed']:\n",
    "        display_name = kernel_config['display_name']\n",
    "        data_for_box.append(results[kernel_name]['mse_per_seed'])\n",
    "        labels_for_box.append(display_name)\n",
    "        colors_for_box.append(line_colors.get(display_name, kernel_config['color']))\n",
    "\n",
    "bp = ax.boxplot(data_for_box, labels=labels_for_box, patch_artist=True)\n",
    "for patch, color in zip(bp['boxes'], colors_for_box):\n",
    "    patch.set_facecolor(color)\n",
    "    patch.set_alpha(0.7)\n",
    "    patch.set_linewidth(0.5)\n",
    "\n",
    "for element in ['whiskers', 'fliers', 'means', 'medians', 'caps']:\n",
    "    plt.setp(bp[element], linewidth=0.5)\n",
    "\n",
    "ax.set_ylabel('MSE [test set]')\n",
    "ax.spines['top'].set_visible(False)\n",
    "ax.spines['right'].set_visible(False)\n",
    "ax.grid(False)\n",
    "ax.tick_params(axis='x', rotation=0)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('./notebooks/analysis/task1_mse_boxplot.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"üìä Saved: task1_mse_boxplot.png (300 DPI)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "302e3400",
   "metadata": {},
   "source": [
    "## 8. Visualization: Kernel Matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cfde734",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot kernel matrices for each kernel type (first seed)\n",
    "# Using same style as callback_gppvae from callbacks.py\n",
    "import pylab as pl\n",
    "pl.rcdefaults()  # Reset to matplotlib defaults for consistent styling\n",
    "\n",
    "n_kernels = len([k for k in results if results[k]['kernel_matrices']])\n",
    "if n_kernels > 0:\n",
    "    fig, axes = pl.subplots(2, 2, figsize=(10, 10))\n",
    "    axes = axes.flatten()\n",
    "    \n",
    "    ax_idx = 0\n",
    "    for kernel_name, kernel_config in KERNEL_CONFIGS.items():\n",
    "        if kernel_name in results and results[kernel_name]['kernel_matrices']:\n",
    "            K = results[kernel_name]['kernel_matrices'][0]  # First seed\n",
    "            \n",
    "            ax = axes[ax_idx]\n",
    "            # Match callback_gppvae style: default colormap, vmin=-0.4, vmax=1, aspect='auto'\n",
    "            im = ax.imshow(K, vmin=-0.4, vmax=1, aspect='auto')\n",
    "            \n",
    "            # Add angle labels\n",
    "            tick_positions = [0, 4, 8, 12, 17]\n",
    "            tick_labels = [f\"{p*20}¬∞\" for p in tick_positions]\n",
    "            ax.set_xticks(tick_positions)\n",
    "            ax.set_xticklabels(tick_labels, fontsize=8)\n",
    "            ax.set_yticks(tick_positions)\n",
    "            ax.set_yticklabels(tick_labels, fontsize=8)\n",
    "            ax.tick_params(labelsize=8)\n",
    "            ax.set_title(kernel_config['display_name'], fontsize=10)\n",
    "            \n",
    "            pl.colorbar(im, ax=ax, fraction=0.046, pad=0.04)\n",
    "            ax_idx += 1\n",
    "    \n",
    "    # Hide unused subplots\n",
    "    for i in range(ax_idx, 4):\n",
    "        axes[i].axis('off')\n",
    "    \n",
    "    pl.tight_layout()\n",
    "    pl.savefig('./notebooks/analysis/task1_kernel_matrices.png', dpi=300, bbox_inches='tight')\n",
    "    pl.show()\n",
    "    print(\"üìä Saved: task1_kernel_matrices.png (300 DPI)\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è No kernel matrices to display\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2016eff",
   "metadata": {},
   "source": [
    "## 9. Visualization: Sample Reconstructions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29c7c461",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get reconstructions from each kernel (best seed by MSE)\n",
    "n_samples = 8\n",
    "n_kernels = len([k for k in results if results[k]['mse_per_seed']])\n",
    "\n",
    "if n_kernels > 0:\n",
    "    fig, axes = plt.subplots(n_kernels + 1, n_samples, figsize=(n_samples * 1.5, (n_kernels + 1) * 1.5))\n",
    "    \n",
    "    # Plot ground truth (first row)\n",
    "    n_total = len(test_data)\n",
    "    sample_stride = max(1, n_total // n_samples)\n",
    "    sample_indices = list(range(0, n_total, sample_stride))[:n_samples]\n",
    "    Y_gt = test_data.Y[sample_indices].numpy().transpose(0, 2, 3, 1)\n",
    "    \n",
    "    for i in range(n_samples):\n",
    "        axes[0, i].imshow(np.clip(Y_gt[i], 0, 1))\n",
    "        axes[0, i].axis('off')\n",
    "    axes[0, 0].set_ylabel('GT', fontsize=10)\n",
    "    \n",
    "    # Plot reconstructions for each kernel\n",
    "    row_idx = 1\n",
    "    for kernel_name, kernel_config in KERNEL_CONFIGS.items():\n",
    "        if kernel_name in results and results[kernel_name]['mse_per_seed']:\n",
    "            # Find best seed\n",
    "            best_seed_idx = np.argmin(results[kernel_name]['mse_per_seed'])\n",
    "            best_run = find_run_folders(CONFIG['results_base'], kernel_config['folder'])[best_seed_idx]\n",
    "            \n",
    "            # Load models\n",
    "            vae, vm, gp = load_models(\n",
    "                best_run, kernel_config, P, Q, \n",
    "                CONFIG['xdim'], device\n",
    "            )\n",
    "            \n",
    "            # Get reconstructions\n",
    "            Y_orig, Y_recon = get_reconstructions(\n",
    "                vae, vm, gp, train_queue, test_queue,\n",
    "                Dt, Wt, Dtest, Wtest, device, n_samples=n_samples\n",
    "            )\n",
    "            \n",
    "            for i in range(n_samples):\n",
    "                axes[row_idx, i].imshow(np.clip(Y_recon[i], 0, 1))\n",
    "                axes[row_idx, i].axis('off')\n",
    "            \n",
    "            mse = results[kernel_name]['mse_per_seed'][best_seed_idx]\n",
    "            axes[row_idx, 0].set_ylabel(f\"{kernel_config['display_name']}\\n({mse:.4f})\", fontsize=8)\n",
    "            row_idx += 1\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('./notebooks/analysis/task1_reconstructions.png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    print(\"üìä Saved: task1_reconstructions.png (300 DPI)\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è No kernels to display\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85749f59",
   "metadata": {},
   "source": [
    "## 10. Visualization: Per-Seed Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e5ee4a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Line plot showing MSE across seeds for each kernel\n",
    "fig, ax = plt.subplots(figsize=(10, 5))\n",
    "\n",
    "# Define consistent colors for each kernel\n",
    "line_colors = {\n",
    "    'Full Rank': '#1f77b4',      # Blue\n",
    "    'Periodic': '#ff7f0e',        # Orange\n",
    "    'SM (Wrapped)': '#9467bd',    # Purple\n",
    "    'SM (Free)': '#8c564b',       # Brown\n",
    "}\n",
    "\n",
    "for kernel_name, kernel_config in KERNEL_CONFIGS.items():\n",
    "    if kernel_name in results and results[kernel_name]['mse_per_seed']:\n",
    "        mse_values = results[kernel_name]['mse_per_seed']\n",
    "        seeds = range(len(mse_values))\n",
    "        display_name = kernel_config['display_name']\n",
    "        color = line_colors.get(display_name, kernel_config['color'])\n",
    "        ax.plot(seeds, mse_values, 'o-', \n",
    "                label=f\"{display_name} (Œº={np.mean(mse_values):.5f})\",\n",
    "                color=color, markersize=8, linewidth=2)\n",
    "\n",
    "ax.set_xlabel('Seed', fontsize=12)\n",
    "ax.set_ylabel('Test MSE', fontsize=12)\n",
    "ax.legend(loc='best')\n",
    "ax.set_xticks(range(5))\n",
    "ax.spines['top'].set_visible(False)\n",
    "ax.spines['right'].set_visible(False)\n",
    "ax.grid(False)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('./notebooks/analysis/task1_per_seed_mse.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"üìä Saved: task1_per_seed_mse.png (300 DPI)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a63e9c32",
   "metadata": {},
   "source": [
    "## 11. Statistical Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15053a87",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import stats\n",
    "\n",
    "# Pairwise t-tests between kernels\n",
    "kernel_names_list = [k for k in KERNEL_CONFIGS if k in results and results[k]['mse_per_seed']]\n",
    "\n",
    "if len(kernel_names_list) >= 2:\n",
    "    print(\"\\nüìä Pairwise T-Tests (p-values)\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    pvalue_matrix = np.ones((len(kernel_names_list), len(kernel_names_list)))\n",
    "    \n",
    "    for i, k1 in enumerate(kernel_names_list):\n",
    "        for j, k2 in enumerate(kernel_names_list):\n",
    "            if i < j:\n",
    "                mse1 = results[k1]['mse_per_seed']\n",
    "                mse2 = results[k2]['mse_per_seed']\n",
    "                \n",
    "                # Paired t-test\n",
    "                t_stat, p_value = stats.ttest_ind(mse1, mse2)\n",
    "                pvalue_matrix[i, j] = p_value\n",
    "                pvalue_matrix[j, i] = p_value\n",
    "                \n",
    "                k1_name = KERNEL_CONFIGS[k1]['display_name']\n",
    "                k2_name = KERNEL_CONFIGS[k2]['display_name']\n",
    "                sig = \"*\" if p_value < 0.05 else \"\"\n",
    "                print(f\"{k1_name} vs {k2_name}: p = {p_value:.4f} {sig}\")\n",
    "    \n",
    "    print(\"\\n* indicates p < 0.05 (statistically significant)\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è Need at least 2 kernels for statistical comparison\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76e5b05c",
   "metadata": {},
   "source": [
    "## 12. Export Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d14ab07d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save detailed results to CSV\n",
    "detailed_results = []\n",
    "\n",
    "for kernel_name, kernel_config in KERNEL_CONFIGS.items():\n",
    "    if kernel_name in results and results[kernel_name]['mse_per_seed']:\n",
    "        for seed_idx, mse in enumerate(results[kernel_name]['mse_per_seed']):\n",
    "            detailed_results.append({\n",
    "                'kernel': kernel_config['display_name'],\n",
    "                'seed': seed_idx,\n",
    "                'test_mse': mse,\n",
    "                'variance_ratio': results[kernel_name]['variance_ratios'][seed_idx],\n",
    "            })\n",
    "\n",
    "detailed_df = pd.DataFrame(detailed_results)\n",
    "detailed_df.to_csv('./notebooks/analysis/task1_detailed_results.csv', index=False)\n",
    "print(\"üìä Saved: task1_detailed_results.csv\")\n",
    "\n",
    "# Save summary to CSV\n",
    "summary_df.to_csv('./notebooks/analysis/task1_summary.csv', index=False)\n",
    "print(\"üìä Saved: task1_summary.csv\")\n",
    "\n",
    "# Display detailed results\n",
    "print(\"\\nüìã Detailed Results:\")\n",
    "print(detailed_df.to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2837ae55",
   "metadata": {},
   "source": [
    "## 13. Callback-Style Plot (like training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43cdcc0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import callback_gppvae directly from callbacks.py\n",
    "from callbacks import callback_gppvae, _compose_multi\n",
    "import pylab as pl\n",
    "\n",
    "def generate_callback_plot_for_kernel(kernel_name, kernel_config, results, \n",
    "                                       train_queue, test_queue,\n",
    "                                       Dt, Wt, Dtest, Wtest, P, Q, xdim, device,\n",
    "                                       output_file):\n",
    "    \"\"\"\n",
    "    Generate a callback-style plot for a kernel using the actual callback_gppvae function.\n",
    "    \"\"\"\n",
    "    if kernel_name not in results or not results[kernel_name]['mse_per_seed']:\n",
    "        return None\n",
    "    \n",
    "    # Reset matplotlib style to default (same as training)\n",
    "    pl.rcdefaults()\n",
    "    \n",
    "    # Use best seed\n",
    "    best_seed_idx = np.argmin(results[kernel_name]['mse_per_seed'])\n",
    "    best_run = find_run_folders(CONFIG['results_base'], kernel_config['folder'])[best_seed_idx]\n",
    "    \n",
    "    # Load models\n",
    "    vae, vm, gp = load_models(\n",
    "        best_run, kernel_config, P, Q, xdim, device\n",
    "    )\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        # Get covariances (same as training code)\n",
    "        X = vm.x().cpu().numpy()\n",
    "        W = vm.v().cpu().numpy()\n",
    "        XX = X @ X.T\n",
    "        WW = W @ W.T\n",
    "        covs = {\"XX\": XX[:20, :20], \"WW\": WW}\n",
    "        \n",
    "        # Encode training data\n",
    "        Zm = encode_dataset(vae, train_queue, device)\n",
    "        \n",
    "        # Compute V matrices\n",
    "        Vt = vm(Dt, Wt).detach()\n",
    "        Vtest = vm(Dtest, Wtest).detach()\n",
    "        \n",
    "        # GP prediction\n",
    "        vs = gp.get_vs()\n",
    "        U, UBi, _ = gp.U_UBi_Shb([Vt], vs)\n",
    "        Kiz = gp.solve(Zm, U, UBi, vs)\n",
    "        Zo_test = vs[0] * Vtest.mm(Vt.transpose(0, 1).mm(Kiz))\n",
    "        \n",
    "        # Sample diverse test samples\n",
    "        n_total = len(test_queue.dataset)\n",
    "        if n_total >= 24:\n",
    "            sample_stride = max(1, n_total // 24)\n",
    "            sample_indices = np.arange(0, n_total, sample_stride)[:24]\n",
    "        else:\n",
    "            sample_indices = np.arange(min(24, n_total))\n",
    "        \n",
    "        sample_indices_tensor = torch.tensor(sample_indices, dtype=torch.long).to(device)\n",
    "        \n",
    "        # Get images - Yv (ground truth), Yr (VAE recon), Yo (GP pred)\n",
    "        Yv = test_queue.dataset.Y[sample_indices].numpy().transpose((0, 2, 3, 1))\n",
    "        \n",
    "        # VAE direct reconstruction\n",
    "        Y_input = test_queue.dataset.Y[sample_indices].to(device)\n",
    "        Zm_test, _ = vae.encode(Y_input)\n",
    "        Yr = vae.decode(Zm_test).data.cpu().numpy().transpose((0, 2, 3, 1))\n",
    "        \n",
    "        # GP prediction reconstruction\n",
    "        Yo = vae.decode(Zo_test[sample_indices_tensor]).data.cpu().numpy().transpose((0, 2, 3, 1))\n",
    "        \n",
    "        imgs = {\"Yv\": Yv, \"Yr\": Yr, \"Yo\": Yo}\n",
    "        \n",
    "        # Create history dict with analysis results (instead of training history)\n",
    "        # We'll create a mock history that shows the seed results\n",
    "        n_seeds = len(results[kernel_name]['mse_per_seed'])\n",
    "        history = {\n",
    "            \"loss\": results[kernel_name]['mse_per_seed'],  # Use MSE per seed as \"loss\"\n",
    "            \"vs\": [[vs.cpu().numpy()[0], vs.cpu().numpy()[1]]] * n_seeds,\n",
    "            \"recon_term\": [0.0] * n_seeds,\n",
    "            \"gp_nll\": [0.0] * n_seeds,\n",
    "            \"mse_out\": results[kernel_name]['mse_per_seed'],\n",
    "            \"mse\": [results[kernel_name]['mean_mse']] * n_seeds,\n",
    "            \"mse_val\": results[kernel_name]['mse_per_seed'],\n",
    "        }\n",
    "    \n",
    "    # Call the actual callback_gppvae function\n",
    "    callback_gppvae(\n",
    "        epoch=n_seeds - 1,  # Use number of seeds as \"epoch\"\n",
    "        history=history,\n",
    "        covs=covs,\n",
    "        imgs=imgs,\n",
    "        ffile=output_file\n",
    "    )\n",
    "    \n",
    "    return output_file\n",
    "\n",
    "print(\"‚úÖ Callback plot function defined (using callback_gppvae from callbacks.py)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f498a68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate callback plots for each kernel (using callback_gppvae from callbacks.py)\n",
    "for kernel_name, kernel_config in KERNEL_CONFIGS.items():\n",
    "    if kernel_name in results and results[kernel_name]['mse_per_seed']:\n",
    "        print(f\"\\nüìä Generating callback plot for {kernel_config['display_name']}...\")\n",
    "        \n",
    "        filename = f'./notebooks/analysis/task1_callback_{kernel_name}.png'\n",
    "        \n",
    "        result = generate_callback_plot_for_kernel(\n",
    "            kernel_name, kernel_config, results,\n",
    "            train_queue, test_queue,\n",
    "            Dt, Wt, Dtest, Wtest, P, Q, CONFIG['xdim'], device,\n",
    "            output_file=filename\n",
    "        )\n",
    "        \n",
    "        if result:\n",
    "            print(f\"   ‚úÖ Saved: {filename}\")\n",
    "            # Display the saved image\n",
    "            from IPython.display import Image, display\n",
    "            display(Image(filename=filename))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e149711",
   "metadata": {},
   "source": [
    "## 14. Final Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "997e09b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"üìä TASK 1 (STANDARD) - FINAL SUMMARY\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(\"\\nüèÜ Kernel Ranking (by Mean Test MSE):\")\n",
    "print(\"-\"*50)\n",
    "\n",
    "ranked = sorted(\n",
    "    [(k, results[k]['mean_mse'], results[k]['std_mse']) \n",
    "     for k in results if results[k]['mse_per_seed']],\n",
    "    key=lambda x: x[1]\n",
    ")\n",
    "\n",
    "for rank, (kernel_name, mean_mse, std_mse) in enumerate(ranked, 1):\n",
    "    display_name = KERNEL_CONFIGS[kernel_name]['display_name']\n",
    "    medal = \"ü•á\" if rank == 1 else \"ü•à\" if rank == 2 else \"ü•â\" if rank == 3 else \"  \"\n",
    "    print(f\"{medal} {rank}. {display_name:15s}: {mean_mse:.6f} ¬± {std_mse:.6f}\")\n",
    "\n",
    "print(\"\\nüìÅ Generated Files:\")\n",
    "print(\"-\"*50)\n",
    "print(\"   - task1_mse_comparison.png\")\n",
    "print(\"   - task1_kernel_matrices.png\")\n",
    "print(\"   - task1_reconstructions.png\")\n",
    "print(\"   - task1_per_seed_mse.png\")\n",
    "print(\"   - task1_detailed_results.csv\")\n",
    "print(\"   - task1_summary.csv\")\n",
    "for k in results:\n",
    "    if results[k]['mse_per_seed']:\n",
    "        print(f\"   - task1_callback_{k}.png\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"‚úÖ Analysis Complete!\")\n",
    "print(\"=\"*70)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.12.10)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
