# GP-VAE Kernel Library

This document describes the different kernel functions available for modeling view correlations in GP-VAE.

## Overview

The `kernels.py` module provides 7 different kernel implementations for modeling correlations between view angles. Each kernel has different properties and is suitable for different scenarios.

## Available Kernels

### 1. **Full Rank Kernel** (`'full'`, `'fullrank'`)
```python
kernel = FullRankKernel(n_views=9)
```
- **Description**: Learns an arbitrary QÃ—Q covariance matrix
- **Parameters**: QÂ²/2 (45 for 9 views)
- **Pros**: Most expressive, no assumptions
- **Cons**: Most parameters, can overfit
- **Use when**: You have lots of data and want maximum flexibility

### 2. **Linear Kernel** (`'linear'`)
```python
kernel = LinearKernel(n_views=9, rank=3)
```
- **Description**: K = V @ V^T where V are learned view embeddings
- **Parameters**: Q Ã— rank (27 for rank=3, 9 views)
- **Pros**: Low-rank structure, original GP-VAE kernel
- **Cons**: Doesn't explicitly use angle information
- **Use when**: Default choice, works well in practice
- **Note**: This is the kernel used in the original Casale et al. (2018) paper

### 3. **RBF Angle Kernel** (`'rbf'`, `'gaussian'`)
```python
kernel = RBFAngleKernel(n_views=9, lengthscale=1.0)
```
- **Formula**: k(Î¸, Î¸') = exp(-(Î¸ - Î¸')Â² / (2â„“Â²))
- **Parameters**: 1 (lengthscale â„“)
- **Pros**: Very smooth, simple
- **Cons**: Not periodic (doesn't know 0Â° = 360Â°)
- **Use when**: Views are not full rotations (e.g., [-90Â°, 90Â°])

### 4. **Periodic Kernel** (`'periodic'`) â­ **RECOMMENDED FOR ROTATIONS**
```python
kernel = PeriodicKernel(n_views=9, lengthscale=1.0)
```
- **Formula**: k(Î¸, Î¸') = exp(-2Â·sinÂ²((Î¸ - Î¸')/2) / â„“Â²)
- **Parameters**: 1 (lengthscale â„“)
- **Pros**: **Correctly handles periodicity** (0Â° = 360Â°), smooth
- **Cons**: None for periodic data
- **Use when**: Modeling full rotations (face views 0-360Â°)
- **Why it's best**: Uses sinÂ²(Î”Î¸/2) which is naturally periodic

### 5. **Rational Quadratic Kernel** (`'rational_quadratic'`, `'rq'`, `'cauchy'`)
```python
kernel = RationalQuadraticKernel(n_views=9, lengthscale=1.0, alpha=1.0)
```
- **Formula**: k(Î¸, Î¸') = (1 + (Î¸ - Î¸')Â² / â„“Â²)^(-Î±)
- **Parameters**: 2 (lengthscale â„“, shape Î±)
- **Pros**: Heavier tails than RBF, mixture of scales
- **Cons**: Not periodic
- **Use when**: You want smooth transitions with occasional long-range correlations

### 6. **MatÃ©rn Kernel** (`'matern'`)
```python
kernel = MaternKernel(n_views=9, lengthscale=1.0, nu=1.5)
```
- **Formula** (Î½=1.5): k(r) = (1 + âˆš3Â·r/â„“) Â· exp(-âˆš3Â·r/â„“)
- **Parameters**: 1 (lengthscale â„“)
- **Pros**: More realistic than RBF, less smooth
- **Cons**: Not periodic
- **Use when**: RBF is too smooth, you want Î½=1.5 (once differentiable) or Î½=2.5
- **Options**: `nu=1.5` (default) or `nu=2.5`

### 7. **Von Mises Kernel** (`'vonmises'`, `'von_mises'`) â­ **RECOMMENDED FOR ANGLES**
```python
kernel = VonMisesKernel(n_views=9, kappa=1.0)
```
- **Formula**: k(Î¸, Î¸') = exp(Îº Â· cos(Î¸ - Î¸'))
- **Parameters**: 1 (concentration Îº)
- **Pros**: **Designed specifically for circular data**, naturally periodic
- **Cons**: None for angular data
- **Use when**: Modeling angles/rotations (alternative to Periodic)
- **Why it's good**: This is the circular analog of the Gaussian distribution

## Quick Start

### Creating a Kernel

```python
from kernels import get_kernel

# Use factory function
kernel = get_kernel('periodic', n_views=9, lengthscale=0.5)

# Or instantiate directly
from kernels import PeriodicKernel
kernel = PeriodicKernel(n_views=9, lengthscale=0.5)
```

### Getting the Covariance Matrix

```python
# Full QÃ—Q matrix
K_full = kernel.get_full_matrix()  # [9, 9]

# Subset for specific views
view_indices = torch.tensor([0, 1, 4, 7])
K_subset = kernel(view_indices)  # [4, 4]
```

### Visualizing Kernels

```python
import matplotlib.pyplot as plt
import torch

kernel = get_kernel('periodic', n_views=9)
K = kernel.get_full_matrix().detach().numpy()

plt.imshow(K, cmap='viridis')
plt.colorbar()
plt.title('Periodic Kernel')
plt.xlabel('View 1')
plt.ylabel('View 2')
plt.show()
```

## Recommendations by Use Case

### For Face View Angles (0Â°, 15Â°, 30Â°, ..., 90Â°)
**Recommended**: `'periodic'` or `'vonmises'`
- These correctly handle the fact that rotations wrap around
- Periodic kernel is smoother, Von Mises is specifically designed for circular data

### For Limited View Range (e.g., only front views -45Â° to +45Â°)
**Recommended**: `'rbf'` or `'matern'`
- No periodicity needed
- RBF for smooth, MatÃ©rn for more realistic

### For Maximum Flexibility (with lots of data)
**Recommended**: `'full'` or `'linear'` with high rank
- Full rank: learns everything from data
- Linear: good compromise between flexibility and regularization

### For Comparing with Original GP-VAE Paper
**Recommended**: `'linear'` with `rank=Q` (number of views)
- This replicates the original Casale et al. (2018) kernel

## Parameters

### Lengthscale (â„“)
- Controls how quickly correlations decay with distance
- **Small â„“**: Nearby views are correlated, distant views are independent
- **Large â„“**: Even distant views are correlated
- Log-parameterized internally: always positive

### Kappa (Îº) - Von Mises only
- Controls concentration around the mean direction
- **Small Îº** (â†’0): Nearly uniform, all views correlated equally
- **Large Îº**: Sharp peak, only nearby views correlated
- Similar role to inverse lengthscale

### Alpha (Î±) - Rational Quadratic only  
- Controls tail heaviness
- **Î± â†’ âˆ**: Becomes RBF kernel
- **Small Î±**: Heavier tails, long-range correlations

### Nu (Î½) - MatÃ©rn only
- Controls smoothness
- **Î½ = 1.5**: Once differentiable (less smooth)
- **Î½ = 2.5**: Twice differentiable (smoother)
- **Î½ â†’ âˆ**: Becomes RBF (infinitely differentiable)

## Integration with GP-VAE

To use these kernels in your GP-VAE training, you'll need to modify `vmod.py` to use a kernel instead of raw embeddings. See the next section for integration code.

## Example: Comparing All Kernels

```python
from kernels import get_kernel
import matplotlib.pyplot as plt
import numpy as np

n_views = 9
angles = np.linspace(0, 2*np.pi, n_views, endpoint=False)

kernels = {
    'Linear': get_kernel('linear', n_views, rank=3),
    'RBF': get_kernel('rbf', n_views, angles=angles),
    'Periodic': get_kernel('periodic', n_views, angles=angles),
    'Von Mises': get_kernel('vonmises', n_views, angles=angles),
    'MatÃ©rn': get_kernel('matern', n_views, angles=angles, nu=1.5),
}

fig, axes = plt.subplots(1, 5, figsize=(20, 4))
for ax, (name, kernel) in zip(axes, kernels.items()):
    K = kernel.get_full_matrix().detach().numpy()
    im = ax.imshow(K, cmap='viridis', vmin=0, vmax=1)
    ax.set_title(name)
    plt.colorbar(im, ax=ax)

plt.tight_layout()
plt.savefig('kernel_comparison.png')
```

## Performance Considerations

**IMPORTANT**: All kernels maintain the same O(NrÂ²QÂ²) complexity as the original GP-VAE! The view kernel is QÃ—Q where Q=9, which is tiny, so the overhead is negligible.

### Complexity Analysis

The GP-VAE uses a **Kronecker-structured kernel**:
```
K_total = vâ‚€Â·(K_object âŠ— K_view) + vâ‚Â·I
```

Where:
- **Object kernel**: Rank-r (typically r=64), size NÃ—N â†’ O(NrÂ²) via Woodbury identity
- **View kernel**: Full QÃ—Q (Q=9), size 9Ã—9 â†’ QÂ² = 81 operations (constant!)

The **Woodbury identity** applies to the object kernel (inverting rÃ—r instead of NÃ—N), not the view kernel. Since Q=9 is small, even a full QÃ—Q kernel adds only ~81 operations, which is negligible compared to the O(NrÂ²) â‰ˆ O(NÂ·4096) cost of the object kernel.

### Performance Table

| Kernel | Parameters | View Matrix Cost | Total Complexity | Works with Woodbury? |
|--------|------------|------------------|------------------|---------------------|
| Legacy (q=Q) | 81 (normalized) | O(QÂ²) = 81 ops | O(NrÂ²QÂ²) âœ… | âœ… YES |
| Full Rank | 45 (triangular) | O(QÂ³) = 243 ops (Cholesky) | O(NrÂ²QÂ²) âœ… | âœ… YES |
| Linear (rank r) | QÃ—r | O(QrÂ²) | O(NrÂ²QÂ²) âœ… | âœ… YES |
| RBF | 1 | O(QÂ²) = 81 ops | O(NrÂ²QÂ²) âœ… | âœ… YES |
| Periodic | 1 | O(QÂ²) = 81 ops | O(NrÂ²QÂ²) âœ… | âœ… YES |
| Rational Quadratic | 2 | O(QÂ²) = 81 ops | O(NrÂ²QÂ²) âœ… | âœ… YES |
| MatÃ©rn | 1 | O(QÂ²) = 81 ops | O(NrÂ²QÂ²) âœ… | âœ… YES |
| Von Mises | 1 | O(QÂ²) = 81 ops | O(NrÂ²QÂ²) âœ… | âœ… YES |

**Key Insight**: Since Q=9 is tiny, the view kernel computation (81-243 operations) is a **negligible constant overhead** compared to the O(NrÂ²) object kernel operations (thousands to millions of ops).

### Speed Comparison

With N=1000 objects, r=64 object rank, Q=9 views:
- **Object kernel cost**: O(NrÂ²) = 1000 Ã— 4096 = **4,096,000 operations**
- **View kernel cost**: O(QÂ²) = 81 = **81 operations** (0.002% overhead!)
- **Cholesky overhead**: O(QÂ³) = 243 extra ops for structured kernels (0.006% overhead!)

**Conclusion**: All kernels have **identical practical speed**. Choose based on **loss/generalization**, not speed!

### Memory Usage

| Kernel | Learnable Parameters | Memory Overhead |
|--------|---------------------|-----------------|
| Legacy (q=Q) | 81 | Minimal |
| Full Rank | 45 | Minimal |
| Linear (rank=3) | 27 | Minimal |
| RBF/Periodic/MatÃ©rn/Von Mises | 1 | **Tiny!** |
| Rational Quadratic | 2 | **Tiny!** |

Where:
- Q = number of views (e.g., 9)
- r = rank (for linear kernel)
- N = number of objects (e.g., 1000)

**Regularization Effect**: Structured kernels with 1-2 parameters provide massive regularization compared to 45-81 free parameters!

## Kernel Comparison Methodology

### Metrics to Track

When comparing different kernels, track these metrics:

#### 1. **Reconstruction Loss**
- **MSE_train**: Mean squared error on training set
- **MSE_val**: Mean squared error on validation set  
- **MSE_out**: Out-of-sample prediction (interpolating missing views)

**Expected behavior:**
- Legacy/FullRank: Lowest train MSE (can overfit)
- Periodic/VonMises: Better val/out MSE (generalize better)

#### 2. **Variance Decomposition**
Track the learned variance components:
- **vâ‚€**: Object-specific variance (shared across views)
- **vâ‚**: View-specific variance (independent noise)
- **Ratio vâ‚€/vâ‚**: How much structure vs noise

**Interpretation:**
- High vâ‚€/vâ‚: Model learned meaningful view structure
- Low vâ‚€/vâ‚: Model treats views as independent noise

#### 3. **Kernel Hyperparameters**
For structured kernels, track learned parameters:
- **Periodic/RBF**: Lengthscale â„“ (how fast correlation decays)
- **Von Mises**: Kappa Îº (concentration)
- **Linear**: Effective rank of V@V^T

**What they mean:**
- Small â„“ or large Îº: Only nearby views correlated
- Large â„“ or small Îº: Even distant views correlated

#### 4. **Generalization Gap**
- **Gap = MSE_train - MSE_val**
- **Smaller gap = better regularization**

**Expected ranking:**
- Structured (1-2 params) < FullRank (45 params) < Legacy (81 params)

### Recommended Plots

#### ğŸ“ˆ **1. Learning Curves**
Plot training and validation loss over epochs for each kernel. Should show:
- Legacy: Lowest training loss, higher validation loss (overfitting)
- Periodic/VonMises: Slightly higher training, better validation (good generalization)

#### ğŸ¨ **2. Learned Kernel Matrices** (Most Insightful!)
Visualize the learned K matrices as heatmaps. Look for:
- **Periodic**: Smooth block diagonal, K[0,8] â‰ˆ 1 (wraparound!)
- **Legacy**: Random structure (no interpretable pattern)
- **VonMises**: Sharp diagonal, smooth decay
- **FullRank**: Arbitrary learned structure

#### ğŸ“Š **3. Variance Decomposition Bar Chart**
Compare vâ‚€, vâ‚, and vâ‚€/vâ‚ ratio across kernels. Higher vâ‚€/vâ‚ indicates the model learned meaningful structure.

#### ğŸ¯ **4. Out-of-Sample Prediction Visualization**
Hold out one view (e.g., 45Â°), predict from others, show ground truth vs prediction for each kernel. Periodic/VonMises should give sharper predictions.

#### â±ï¸ **5. Training Time Comparison**
Verify all kernels have similar training time (proof that Q=9 overhead is negligible!).

### Expected Results

| Metric | Legacy | FullRank | Periodic | Von Mises |
|--------|--------|----------|----------|-----------|
| **Train MSE** | **Lowest** | Low | Medium | Medium |
| **Val MSE** | Medium | Medium | **Lowest** | **Lowest** |
| **Out-of-sample** | Worst | Bad | **Best** | **Best** |
| **Gap (overfit)** | Largest | Large | Small | Small |
| **vâ‚€/vâ‚** | Medium | Medium | **High** | **High** |
| **Speed** | â‰ˆ | â‰ˆ | â‰ˆ | â‰ˆ |
| **Parameters** | 81 | 45 | 1 | 1 |
| **Interpretability** | Low | Low | **High** | **High** |

**Winner**: Periodic or Von Mises for rotation data (better generalization, fewer parameters, interpretable structure)

## References

1. **Original GP-VAE**: Casale et al. (2018) "Gaussian Process Prior Variational Autoencoders"
2. **Periodic Kernel**: MacKay (1998) "Introduction to Gaussian Processes"
3. **Von Mises**: Mardia & Jupp (2000) "Directional Statistics"
4. **MatÃ©rn**: Rasmussen & Williams (2006) "Gaussian Processes for Machine Learning"

## Testing

Run the module directly to generate visualizations:
```bash
cd GPPVAE/pysrc/faceplace
python kernels.py
```

This will create `kernel_comparison.png` showing all kernel matrices.


