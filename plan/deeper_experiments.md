# Deeper Experiments: Favoring Structured Kernels

## Problem Statement
Current metrics show FullRank kernel performing as well or better than structured kernels (Periodic/VonMises/MatÃ©rn). This is because the current setup has:
- âœ… Lots of data (9 views Ã— ~200 people = ~1800 training samples)
- âœ… All views present during training
- âœ… Interpolation within seen range

â†’ FullRank can memorize 45 parameters and win

## Goal
Design experiments where structured kernels' inductive biases (smoothness, periodicity) provide clear advantages.

---

## ğŸ¥‡ Tier 1: EXTREME Regularization Needed
*(Structured kernels will dominate)*

### 1. Hard Held-Out Views â­â­â­â­â­
**Setup**: Train on central views only (-30Â° to +30Â°), test on extreme poses (Â±60Â°, Â±90Â°)

**Why it works**: 
- FullRank has no inductive bias about angular smoothness
- Must extrapolate far from training distribution
- Periodic/VonMises know angles wrap around smoothly

**Expected winner**: Periodic/VonMises

**Expected performance**:
- Periodic MSE_out: ~0.05
- FullRank MSE_out: ~0.15+ (3Ã— worse!)

**Implementation difficulty**: Easy

**Impact**: ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ **HIGHEST**

---

### 2. Few-Shot Learning (Very Few Images per Identity) â­â­â­â­â­
**Setup**: Only 3-5 images per person instead of 9 views

**Why it works**:
- FullRank needs more data to learn 45 parameters
- Periodic/VonMises only have 1 parameter (lengthscale)
- Strong regularization critical with sparse data

**Expected winner**: Periodic/VonMises (only 1 parameter to learn)

**Expected performance**: Periodic wins by 20-40% on MSE_out

**Implementation difficulty**: Medium (need to modify data sampling)

**Impact**: ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ **HIGHEST**

---

### 3. Train on Fewer Identities â­â­â­â­
**Setup**: Use only 20-50 people instead of all ~200

**Why it works**:
- Less data = stronger need for regularization
- Structured kernels generalize better with limited samples

**Expected winner**: MatÃ©rn/Periodic (smooth interpolation helps)

**Expected performance**: Periodic/MatÃ©rn win by 10-20% on MSE_out

**Implementation difficulty**: Easy

**Impact**: ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ **VERY HIGH**

---

## ğŸ¥ˆ Tier 2: STRONG Regularization Advantage
*(Structured kernels likely win)*

### 4. Cross-Identity View Prediction â­â­â­â­
**Setup**: Train on identities A-N, test on new identities M-Z with missing views

**Why it works**:
- Tests if kernel generalizes to unseen people
- View structure should transfer across identities

**Expected winner**: Periodic/VonMises (view structure transfers)

**Implementation difficulty**: Easy (already have train/val split structure)

**Impact**: ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ **VERY HIGH** - True out-of-distribution test

---

### 5. Sparse View Sampling (Non-Uniform) â­â­â­â­
**Setup**: Train on views [0, 2, 4, 6, 8] (skip every other), predict views [1, 3, 5, 7]

**Why it works**:
- Forces interpolation between distant views (30Â° gaps)
- Smooth kernels interpolate better

**Expected winner**: Periodic/MatÃ©rn (smooth interpolation)

**Implementation difficulty**: Easy

**Impact**: ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ **HIGH** - Tests interpolation directly

---

### 6. Added Noise/Corruption â­â­â­
**Setup**: Add Gaussian noise to images or latent codes during training

**Why it works**:
- Noisy data â†’ need stronger priors
- Structured kernels won't overfit noise patterns

**Expected winner**: Periodic/VonMises (won't overfit noise)

**Implementation difficulty**: Very easy

**Impact**: ğŸ”¥ğŸ”¥ğŸ”¥ **MEDIUM-HIGH**

---

## ğŸ¥‰ Tier 3: MODERATE Regularization Advantage
*(Structured kernels should win, but smaller margin)*

### 7. Lower Latent Dimensionality â­â­â­
**Setup**: Train VAE with zdim=16 instead of zdim=32

**Why it works**:
- Less capacity â†’ need better structure
- Efficient parameterization matters more

**Expected winner**: Periodic (most efficient)

**Implementation difficulty**: Hard (need to retrain VAE)

**Impact**: ğŸ”¥ğŸ”¥ğŸ”¥ **MEDIUM**

---

### 8. Asymmetric View Distribution â­â­â­
**Setup**: Train on more left views than right (e.g., 2Ã— more left samples)

**Why it works**:
- Imbalanced data â†’ need to generalize better
- Symmetry assumption helps

**Expected winner**: Periodic/VonMises (symmetry assumption helps)

**Implementation difficulty**: Medium

**Impact**: ğŸ”¥ğŸ”¥ğŸ”¥ **MEDIUM**

---

### 9. Early Stopping â­â­
**Setup**: Stop training after 20-30 epochs instead of 100

**Why it works**:
- Less time to overfit
- Simpler models learn faster

**Expected winner**: Periodic/VonMises (learn faster with 1 param)

**Implementation difficulty**: Trivial

**Impact**: ğŸ”¥ğŸ”¥ **LOW-MEDIUM**

---

## ğŸ“Š RANKED SUMMARY

| Rank | Scenario | Difficulty | Impact | Best Kernel | Expected Advantage |
|------|----------|-----------|--------|-------------|-------------------|
| **1** | Hard held-out views (-30â†’30 train, Â±60/90 test) | Easy | ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ | Periodic/VonMises | 3Ã— better MSE_out |
| **2** | Few-shot (3-5 images per identity) | Medium | ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ | Periodic/VonMises | 20-40% better |
| **3** | Fewer identities (20-50 people) | Easy | ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ | MatÃ©rn/Periodic | 10-20% better |
| **4** | Cross-identity (train A-N, test M-Z) | Easy | ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ | Periodic/VonMises | 15-30% better |
| **5** | Sparse views (skip every other) | Easy | ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ | Periodic/MatÃ©rn | 15-25% better |

---

## ğŸ’¡ Recommended Starting Point

**Start with Experiment #1: Hard Held-Out Views**

**Reasons**:
1. âœ… Easiest to implement (just filter view indices)
2. âœ… Highest expected impact (3Ã— performance difference)
3. âœ… Most interpretable (extrapolation vs interpolation)
4. âœ… Directly tests the core hypothesis (angular smoothness)

---

## ğŸ¯ Core Principle

All successful scenarios share:
```
Less data + Harder task = Need for better inductive bias

FullRank: "I'll memorize the 45 correlations I see"
Periodic: "I know angles are smooth and wrap around"
```

**When you have**:
- âœ… Lots of data per identity (9 views Ã— 200 people)
- âœ… All views present during training
- âœ… Interpolation within seen range
â†’ **FullRank can memorize and wins**

**When you have**:
- âŒ Sparse views (3-5 instead of 9)
- âŒ Extrapolation to unseen angles
- âŒ Fewer identities
â†’ **Periodic/VonMises force smooth structure and win decisively!** ğŸ†

---

## ğŸ“ Notes on Variance Components

The GP model learns two variance components:
- **vâ‚€ (Object Variance)**: How much latent variation comes from object identity
- **vâ‚ (Noise Variance)**: Unexplained variation (views, noise, other factors)

**Variance Ratio**: `vâ‚€ / (vâ‚€ + vâ‚)`
- Close to 1.0 â†’ GP successfully learned object structure âœ…
- Close to 0.5 â†’ Object and noise equally important
- Close to 0.0 â†’ Model failed to learn structure âŒ

Higher vâ‚€ relative to vâ‚ = Better disentanglement of object identity from other factors.
